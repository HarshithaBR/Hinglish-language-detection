{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hsNjJQQlfMIb",
        "outputId": "46ad88eb-9953-4a43-9a42-7e4197a8f198"
      },
      "id": "hsNjJQQlfMIb",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e8a5dd5",
      "metadata": {
        "id": "1e8a5dd5"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import shutil\n",
        "# from typing_extensions import TypeAliasType\n",
        "\n",
        "try:\n",
        "    sys.path.append(os.path.join(os.path.dirname(__file__), '../'))\n",
        "except:\n",
        "    sys.path.append(os.path.join(os.getcwd(), '../'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import sentiwordnet as swn\n",
        "from collections import Counter\n",
        "from sklearn import feature_extraction"
      ],
      "metadata": {
        "id": "sa3fpJmZPPjT"
      },
      "id": "sa3fpJmZPPjT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('sentiwordnet')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOFu8to-PW6-",
        "outputId": "8c6360f8-4447-4b84-cbe7-79a373d2ef15"
      },
      "id": "pOFu8to-PW6-",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package sentiwordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87837cfd",
      "metadata": {
        "id": "87837cfd",
        "outputId": "890bd61f-73d3-42f3-8504-7138e34131f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7ba590108030>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import pickle\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix, precision_score, recall_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "\n",
        "# from src import data\n",
        "\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28b18501",
      "metadata": {
        "id": "28b18501"
      },
      "outputs": [],
      "source": [
        "# train_file = '/content/drive/MyDrive/NLP_Project/IIITH_Codemixed.txt'\n",
        "train_file = '/content/drive/MyDrive/nlp_project/HIT-ACL2021-Codemixed-Representation/data/hindi_sentiment/IIITH_Codemixed.txt'\n",
        "df = pd.read_csv(train_file, sep='\\t', header=None, usecols=[1,2])\n",
        "df.columns = ['text', 'category']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = []\n",
        "with open(train_file, 'r', encoding='utf-8') as file:\n",
        "    for line in file:\n",
        "      parts = line.strip().split('\\t')\n",
        "      if len(parts) > 0:\n",
        "            sentences.append(parts[1])"
      ],
      "metadata": {
        "id": "XYl3QXdpopqj"
      },
      "id": "XYl3QXdpopqj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "capital_words_count = []\n",
        "for sentence in sentences:\n",
        "    capital_words = re.findall(r'\\b[A-Z]+\\b', sentence)\n",
        "    capital_words_count.append(len(capital_words))"
      ],
      "metadata": {
        "id": "e2XQYd6vpL9c"
      },
      "id": "e2XQYd6vpL9c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extended_words_count = []\n",
        "for sentence in sentences:\n",
        "    extended_words = re.findall(r'\\b\\w*([a-zA-Z])\\1\\w*\\b', sentence)\n",
        "    # print(extended_words)\n",
        "    extended_words_count.append(len(extended_words))"
      ],
      "metadata": {
        "id": "OAHVqybMpL51"
      },
      "id": "OAHVqybMpL51",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "exclamation_at_end = []\n",
        "for sentence in sentences:\n",
        "    exclamation_at_end.append(int(sentence.endswith('!')))"
      ],
      "metadata": {
        "id": "R9Lepsy3pL2a"
      },
      "id": "R9Lepsy3pL2a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "repeated_punctuation_count = []\n",
        "\n",
        "punctuation_pattern = r'(\\W)\\1+'\n",
        "\n",
        "for sentence in sentences:\n",
        "    repeated_punctuation = re.findall(punctuation_pattern, sentence)\n",
        "    repeated_punctuation_count.append(len(repeated_punctuation))\n",
        "\n",
        "print(repeated_punctuation_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFs50q_TpLy6",
        "outputId": "2cdb3c08-63e5-40b8-ee94-2f5a8fd91255"
      },
      "id": "NFs50q_TpLy6",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 2, 2, 0, 2, 1, 1, 0, 5, 0, 0, 0, 0, 0, 1, 3, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 3, 0, 0, 0, 0, 1, 0, 0, 3, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 5, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 2, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 4, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 2, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 2, 0, 0, 2, 0, 0, 2, 0, 0, 0, 0, 5, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0, 7, 0, 0, 0, 0, 0, 1, 0, 0, 2, 1, 2, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 3, 0, 0, 1, 0, 0, 0, 0, 0, 3, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 3, 2, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 3, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 3, 0, 2, 0, 2, 2, 0, 0, 4, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 3, 0, 0, 0, 1, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 2, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 3, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 3, 0, 0, 0, 0, 0, 9, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 2, 0, 0, 1, 0, 0, 4, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 36, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 0, 1, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 2, 0, 2, 0, 4, 0, 0, 0, 0, 0, 2, 0, 0, 7, 1, 0, 1, 1, 2, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 3, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 2, 2, 0, 1, 4, 2, 0, 92, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 1, 1, 0, 0, 1, 0, 0, 3, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 3, 0, 1, 1, 0, 1, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 2, 0, 1, 0, 0, 0, 1, 0, 3, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 1, 0, 1, 0, 10, 0, 1, 0, 0, 0, 1, 3, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 3, 0, 2, 0, 0, 0, 0, 9, 0, 8, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 3, 0, 2, 0, 0, 0, 0, 0, 1, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 1, 0, 1, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 2, 0, 3, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 1, 2, 1, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 4, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 1, 4, 5, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 3, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 1, 2, 0, 1, 0, 0, 1, 0, 2, 0, 0, 0, 0, 1, 0, 2, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 2, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 4, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 1, 0, 3, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 3, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 4, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 6, 2, 3, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 1, 0, 2, 0, 0, 2, 1, 1, 0, 0, 2, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 2, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 6, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 2, 0, 0, 1, 1, 4, 0, 1, 0, 0, 0, 3, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 2, 0, 0, 0, 0, 0, 1, 0, 1, 3, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 2, 0, 0, 0, 1, 0, 1, 0, 3, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 3, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 3, 1, 0, 0, 0, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 1, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 1, 0, 3, 0, 0, 0, 2, 0, 0, 0, 1, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 1, 1, 3, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 2, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 3, 1, 0, 0, 0, 0, 0, 1, 0, 5, 0, 0, 0, 0, 0, 0, 1, 2, 1, 0, 0, 0, 1, 0, 1, 2, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 2, 1, 4, 0, 0, 2, 0, 0, 0, 3, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 3, 1, 0, 0, 0, 2, 0, 1, 1, 0, 0, 1, 0, 0, 3, 0, 1, 0, 2, 1, 0, 1, 2, 1, 2, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 2, 0, 0, 0, 2, 0, 0, 0, 1, 0, 1, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 3, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 3, 0, 1, 0, 0, 0, 0, 0, 1, 3, 0, 2, 0, 2, 1, 0, 0, 0, 0, 1, 1, 0, 0, 2, 0, 0, 0, 0, 1, 2, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 2, 1, 0, 1, 0, 2, 1, 3, 1, 0, 0, 2, 1, 2, 0, 0, 0, 3, 1, 0, 9, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 1, 2, 0, 2, 2, 0, 2, 0, 0, 0, 0, 0, 0, 3, 1, 1, 0, 4, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 2, 4, 0, 2, 0, 1, 2, 2, 1, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 1, 3, 0, 0, 0, 0, 0, 2, 0, 1, 2, 1, 0, 1, 0, 1, 0, 0, 1, 1, 6, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 1, 0, 2, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 3, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 9, 0, 0, 6, 0, 0, 4, 7, 1, 0, 1, 0, 0, 3, 1, 4, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 3, 3, 0, 0, 0, 2, 0, 0, 1, 0, 0, 3, 0, 0, 0, 0, 0, 1, 3, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 0, 0, 0, 0, 0, 1, 2, 0, 1, 1, 0, 1, 1, 0, 1, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 2, 0, 1, 0, 1, 0, 2, 0, 0, 3, 0, 1, 0, 0, 0, 6, 2, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 4, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 4, 0, 0, 3, 0, 0, 0, 0, 3, 0, 2, 1, 0, 2, 0, 2, 2, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 2, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 4, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 2, 3, 2, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 1, 1, 2, 0, 2, 0, 0, 0, 0, 0, 1, 0, 1, 0, 2, 0, 0, 1, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 2, 3, 1, 0, 1, 0, 1, 0, 3, 0, 0, 0, 0, 0, 3, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 1, 0, 3, 0, 2, 2, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 2, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 2, 0, 2, 2, 0, 0, 0, 2, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 1, 0, 1, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sentiment_scores(word_list):\n",
        "    sentiment_scores_pos = sentiment_scores_neg = 0\n",
        "    for word in word_list:\n",
        "        synsets = list(swn.senti_synsets(word))\n",
        "        if synsets:\n",
        "            pos_score = synsets[0].pos_score()\n",
        "            neg_score = synsets[0].neg_score()\n",
        "            sentiment_scores_pos += pos_score\n",
        "            sentiment_scores_neg += neg_score\n",
        "    return sentiment_scores_pos,sentiment_scores_neg"
      ],
      "metadata": {
        "id": "Q7XR0LKMpLrm"
      },
      "id": "Q7XR0LKMpLrm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_sentiment_score = []\n",
        "neg_sentiment_score = []\n",
        "\n",
        "punctuation_pattern = r'(\\W)\\1+'\n",
        "\n",
        "for sentence in sentences:\n",
        "    pos_sentiment,negetive_sentiment = get_sentiment_scores(sentence.split(' '))\n",
        "    # print(pos_sentiment)\n",
        "    pos_sentiment_score.append((pos_sentiment))\n",
        "    neg_sentiment_score.append((negetive_sentiment))\n",
        "\n",
        "\n",
        "print(pos_sentiment_score)\n",
        "print(neg_sentiment_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGmKvuKjpqQg",
        "outputId": "50201739-d51c-4c1a-fe19-bf8f0ffb8199"
      },
      "id": "MGmKvuKjpqQg",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.0, 1.375, 0.0, 0, 0.0, 0, 1.0, 1.0, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.375, 0.5, 1.375, 0.0, 0.375, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0.75, 0.625, 0.75, 0.5, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.375, 0.0, 0.0, 0, 0.25, 0.375, 0.0, 0.0, 0.5, 0.0, 0.0, 0.75, 0.0, 0.625, 0.0, 0.5, 0.0, 0.0, 0.25, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.375, 0, 0.25, 0.5, 0, 0.0, 0.0, 0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.625, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.375, 0.0, 0.5, 0, 0.625, 0, 0.25, 0.75, 1.125, 0, 1.125, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.5, 0.25, 0.875, 0.375, 1.0, 0.0, 0.5, 0.0, 0.5, 0.875, 0.0, 0.0, 0.0, 1.125, 0.0, 0.0, 0.25, 0.0, 0.25, 0.5, 1.0, 0.0, 0.0, 0.625, 0.0, 0.375, 0.0, 0.25, 0.0, 0.0, 0.125, 0.0, 0.0, 0, 0.0, 0.375, 0.5, 0.5, 0.0, 0.25, 0.0, 0.0, 0.0, 0, 0.875, 0.375, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.5, 0.0, 0.0, 0.25, 0.25, 0.0, 0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.5, 0.0, 0.0, 0.625, 0.0, 0.0, 0.0, 0.75, 0.0, 0.375, 0.0, 0.0, 0.125, 0.0, 1.25, 0.25, 0.0, 1.625, 0.25, 0.0, 0.0, 0.0, 0.625, 0.25, 0.5, 0.0, 1.125, 0.0, 0.25, 0.5, 0.375, 0.0, 0.25, 0.0, 0.0, 0.5, 0.0, 0, 0.0, 0.0, 0.625, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0.25, 0.5, 0.25, 0.0, 0.375, 0.0, 0.25, 0.0, 0, 0.25, 0.0, 0.625, 0.875, 0.0, 0.0, 0.0, 0.875, 0.125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.25, 0.0, 0.375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.125, 0.0, 0.5, 0.0, 0.125, 0.0, 0.0, 0.0, 0.25, 0.0, 0.375, 0.0, 0.25, 0.25, 0.25, 0.0, 0.0, 0.625, 0, 0.0, 0.25, 0.125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.375, 0.0, 0.0, 0.0, 0.0, 0.75, 0.0, 0.25, 0.25, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.25, 1.125, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.458, 0, 0.375, 0.25, 0.0, 0.25, 0.75, 0.0, 0.375, 0.0, 0.25, 0.25, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 1.25, 0.25, 0.25, 0.0, 0.0, 0.25, 0.0, 0.0, 0.5, 0.375, 0.625, 0, 0.25, 0.25, 0.0, 0.375, 0.0, 0.0, 0.25, 0.0, 0.125, 0.0, 0.375, 1.125, 0.0, 0.0, 0.125, 0.0, 0.0, 1.125, 0.0, 0.0, 0.625, 0.75, 0.0, 0, 0.0, 0.0, 0.25, 0.25, 0.25, 0.25, 0.25, 0, 0.5, 0.0, 0.0, 0.75, 0.25, 0.0, 0.0, 0.125, 0.5, 0.25, 0.0, 0.375, 0, 0.5, 0.25, 0.0, 0.0, 0.25, 0.25, 0.0, 0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.625, 0.0, 0.375, 0.75, 0.25, 0.25, 0.0, 0.25, 0.375, 0.0, 0.25, 0.25, 0.0, 1.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.75, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.875, 0.0, 0.0, 0.0, 0.125, 0, 0.0, 0.0, 0.875, 0.0, 0.0, 0.25, 0.125, 0.0, 0.0, 0.625, 0.0, 0.0, 0.0, 0.875, 0.0, 0.0, 0.625, 0.0, 0.25, 0.0, 0.0, 0.25, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.875, 0, 0.25, 0, 0.125, 0.0, 0.0, 0.875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.125, 0, 0.5, 0.0, 0.0, 3.75, 0.0, 0.25, 0.0, 0.0, 0.375, 0.0, 0.0, 0.0, 0.5, 0.125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.875, 0.0, 0.0, 0.0, 0.875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.75, 0.5, 0.0, 0.0, 0.0, 0.0, 0.25, 0, 0.0, 0.625, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.375, 0.0, 0.25, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.25, 0, 0.375, 0.625, 0.0, 0.0, 0.0, 0, 0.875, 0.0, 0.25, 0.0, 0.0, 0, 0, 0.0, 0.0, 0, 0, 0.0, 0.625, 0.0, 0.0, 0.0, 0.625, 0.25, 0.0, 0.0, 0.25, 0.5, 0.875, 0.0, 0.0, 0.0, 0, 0.25, 0.0, 0.0, 0.25, 0.0, 0.125, 0.125, 0.0, 0, 0.25, 0.0, 0.0, 0.0, 0.125, 0.0, 0.125, 0.25, 0.25, 0.0, 0.25, 0.875, 0.0, 0.5, 0.0, 0.0, 0.25, 0.0, 0.0, 0.25, 0.0, 0.75, 0.0, 0.0, 0.5, 0.875, 0.875, 0.0, 0.25, 0.0, 0.0, 1.375, 0.0, 0.125, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 8.0, 0.125, 0.0, 0.0, 0.0, 1.0, 0, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0.25, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0, 0.125, 0.0, 0.0, 0.25, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 1.5, 0.0, 0.0, 0.0, 0.75, 0.0, 0.25, 0.25, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.375, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0, 0.0, 0.25, 0.0, 0.0, 1.0, 1.875, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.25, 0.625, 0.25, 0, 0.25, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 1.375, 0.25, 0.0, 0.0, 0.0, 0.125, 0.0, 0, 0.25, 0.0, 0.125, 0.625, 0.125, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.25, 0.0, 0.375, 0.0, 0.375, 0.0, 0.375, 0.0, 0.625, 0.0, 0.25, 0, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.625, 0.0, 0.625, 0.0, 0.0, 0.0, 0, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.875, 0.25, 0.125, 0.375, 0.0, 0.625, 0.0, 0.0, 0.0, 0.125, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.125, 0.0, 0.625, 0.25, 0.25, 0.0, 0.25, 0.25, 0.0, 0, 0.25, 0, 0, 0.5, 0.75, 0.25, 0.0, 0.0, 0, 0.375, 0.0, 0.25, 0, 0.5, 0.25, 0.0, 0.875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.75, 0.0, 0.25, 0.0, 0.0, 0.0, 2.5, 0.0, 0, 0.25, 0.125, 0.25, 0.5, 0.0, 0.5, 0.625, 0.125, 0.0, 0, 0.5, 0.0, 0.0, 0.75, 0.5, 0.0, 0.0, 0.0, 0.875, 0.5, 0.75, 0.25, 0.5, 0.0, 0.25, 0.0, 0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.75, 0.125, 0, 1.125, 0.0, 0, 0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.25, 0.0, 0.0, 0, 0, 0.0, 0.375, 0.25, 0.5, 0.25, 0.25, 0.0, 0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.5, 0, 0.375, 0.25, 0.0, 0.0, 0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.875, 0.0, 0.375, 0, 0.25, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0.5, 0.25, 0.0, 1.5, 0.75, 0.0, 0.25, 0.0, 3.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 1.0, 0, 0.0, 0.0, 0.0, 1.25, 0.0, 0.0, 0.0, 0.25, 0, 0.0, 0.75, 0.25, 0.0, 0.0, 0.25, 0, 0.25, 0.0, 0.0, 0.0, 0.5, 0.25, 0.0, 0.0, 0.25, 0, 0.0, 0.0, 0.75, 0.0, 0.0, 0, 0.625, 0.125, 0.125, 0.25, 0.0, 0.0, 0.0, 0.625, 0.0, 0.625, 0.0, 0.125, 0.625, 0.625, 0.0, 0.25, 0.625, 0.625, 0.5, 0.25, 0.875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0, 1.0, 0.125, 0.0, 0.75, 0.5, 0.0, 0.0, 0, 0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.125, 0.25, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.25, 0.375, 0.0, 0.0, 0.0, 0.0, 0.625, 0.0, 0, 0.25, 0.625, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.25, 0.75, 0.75, 0.0, 0.25, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.125, 0.875, 0.0, 0.0, 0, 0.0, 0.0, 0.25, 0.5, 0.125, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.75, 0.0, 0.25, 0.0, 0.0, 0, 0.25, 0.0, 0.0, 0.875, 0.125, 0.75, 0.0, 0.125, 0.25, 0.0, 0, 0.0, 0, 1.25, 0.125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 1.875, 0, 2.0, 0.125, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0, 1.25, 0.5, 0.0, 0.0, 0.375, 0.0, 0.25, 0, 1.125, 0.0, 0.0, 0, 0.0, 0.0, 0, 0.0, 0.0, 0.875, 0.625, 0.25, 0, 0, 0.25, 0, 0.625, 0.125, 0.0, 0, 0.0, 0.75, 0.0, 0, 1.375, 0.0, 0.5, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.75, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.125, 0.0, 0.0, 0.125, 0, 0.0, 0.0, 0.5, 0.25, 0.25, 0.5, 0.0, 0.0, 0.0, 0.625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.125, 0.125, 0.0, 0.0, 0.125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.625, 0, 0.25, 0.0, 0, 0.0, 0.5, 0.0, 2.25, 0.875, 0.0, 0.0, 0.0, 0.625, 0.0, 0.0, 1.375, 0.25, 0.875, 0.375, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 1.125, 0.0, 0.25, 0.125, 0, 0.5, 0, 0.0, 0.625, 0.125, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.25, 2.375, 0.0, 0.0, 1.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 1.125, 0.0, 0.0, 0.0, 0.0, 0.25, 0, 0.0, 0.25, 0.25, 0.25, 0.0, 0.0, 0.0, 0, 0.25, 0, 0.0, 0, 0.0, 0.0, 0.125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.25, 0.25, 0.875, 0, 0.0, 0.25, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.875, 1.5, 0.875, 0.0, 0.0, 0.125, 0.5, 0.75, 0.5, 0.0, 0.0, 0, 3.125, 0.125, 0.0, 0, 0.0, 0, 0, 0.5, 0.0, 0.0, 0.0, 0.125, 0.5, 1.25, 0.0, 0.0, 0.875, 0.0, 0.0, 0.0, 2.5, 0.125, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0, 1.125, 0.0, 0.625, 0.125, 0.0, 0.25, 0.25, 1.0, 0.625, 0.0, 0.0, 0.375, 0.0, 0.0, 0.0, 0.625, 0.0, 0.25, 0.0, 0.875, 0.25, 0.0, 0.625, 0.0, 0.25, 0.0, 0.0, 0.0, 0.875, 0.5, 0.25, 0.0, 0.0, 0.125, 0.125, 0.0, 0.875, 0.0, 0.5, 0.0, 0.0, 0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.625, 0.25, 0.0, 0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.625, 0.0, 1.25, 0, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.125, 0.625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.75, 0, 0.0, 0.0, 0.375, 0, 0.0, 0.25, 0.0, 0.125, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 1.375, 0.0, 1.125, 0.25, 0.25, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.875, 0.0, 0, 0.5, 0.0, 0.625, 0.25, 0.0, 0.0, 0.25, 0.0, 0.25, 0.375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.625, 0.0, 0, 0.625, 0, 0.0, 0.25, 0.125, 0.0, 0.0, 0.5, 0.0, 0.0, 0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.25, 0.0, 0, 0.0, 0.5, 0.25, 0.0, 0.5, 0.5, 3.125, 0, 0.0, 0.0, 0.0, 1.25, 0.0, 0, 0.0, 1.0, 0, 0.5, 0.0, 0.5, 0.25, 0, 3.75, 0.0, 0.25, 0.0, 0.0, 0.25, 0.0, 0.0, 0, 0.0, 0.5, 0, 0.25, 0.125, 0.0, 0.125, 0.25, 0.25, 0.0, 0.25, 0.625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.25, 0.125, 1.25, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0.75, 0.0, 0.125, 0.0, 0.125, 0.25, 0.875, 0.0, 0.625, 0.0, 1.0, 0.0, 0.0, 0.125, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0, 0.375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.75, 0.0, 0.0, 0.0, 0.25, 0.875, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0, 0, 0.875, 0.0, 0.5, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.375, 0.125, 0.125, 0.0, 0.375, 0.0, 0.75, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.25, 0.0, 0.0, 0.25, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.25, 0.0, 0.125, 0.0, 0.5, 0.0, 0.0, 1.0, 0.25, 0.0, 0.0, 0.0, 0.875, 0.0, 0.5, 0.25, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.125, 0.25, 0.0, 0.25, 0.0, 0.5, 0.0, 0.0, 0.25, 0.0, 0.0, 0.125, 0.0, 0.0, 0.25, 0.0, 0.0, 0.25, 0.0, 0.25, 0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 1.375, 0.0, 0.375, 0.125, 1.0, 0.0, 0.25, 0.25, 0.0, 0.375, 0.0, 0.875, 0.0, 0.25, 0.0, 0.0, 0, 0.0, 0.25, 0, 0.0, 0.375, 0.375, 0.875, 0.0, 0.625, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.625, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.5, 0.625, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.5, 0, 0, 0.25, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.75, 0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0, 0.0, 0, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.375, 0.0, 0.0, 0, 0.0, 0.5, 0.25, 0.0, 0.0, 0, 0, 0.0, 0.0, 0, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.125, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.5, 0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0.25, 0.0, 0.625, 0.0, 0.0, 0.5, 0.875, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.25, 0.25, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 1.625, 0, 0.0, 0.0, 0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 1.125, 0.0, 0.0, 0.0, 0.125, 0.0, 0, 0.0, 0, 0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.25, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.375, 0, 0.0, 0.0, 0.0, 0.625, 0.5, 0.0, 0.0, 2.75, 0.0, 0, 0, 0.25, 0, 0, 0, 0.25, 0.25, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.875, 1.0, 0.0, 0.0, 0.0, 0, 0.0, 0, 0.0, 0.0, 0.0, 0.125, 0.0, 0.5, 0, 0.0, 0.0, 0.625, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0.125, 0.0, 0.0, 0.0, 0.375, 0.0, 0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.875, 0.25, 0.25, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0.375, 0.0, 0.0, 0.25, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0, 0.0, 1.625, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.125, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.375, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.75, 0.0, 0.0, 0.0, 0.0, 0.25, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.25, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.375, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0, 0.0, 0.375, 0.0, 0.625, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.625, 0.625, 0.0, 0, 0.875, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0, 0, 0.0, 0.25, 0.625, 0.0, 0, 0.25, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.625, 0.0, 0.0, 0.0, 0.0, 0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0, 0.0, 0.25, 0.0, 0.25, 0.0, 0.0, 0.25, 0, 0.0, 0.25, 0.25, 0.0, 0, 0.0, 0.0, 0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.875, 0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.75, 0.0, 0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.125, 0.0, 0.25, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 1.25, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.875, 0, 0.0, 0, 0.5, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.75, 0.0, 0.0, 0.75, 0.0, 0.0, 0.0, 0, 0, 0.0, 0.0, 0.0, 0.375, 0.0, 0.0, 0.625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.75, 0.0, 0.375, 0.75, 0.25, 0.0, 0.0, 0.625, 0.0, 0.0, 0.0, 0.625, 0.0, 0.0, 0.25, 0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.25, 0.0, 0.0, 0.25, 0.0, 0.0, 0.25, 0.5, 0.0, 0.0, 0.0, 0, 0.25, 0.875, 0.0, 0.875, 0.25, 0.0, 0.0, 0.0, 0.25, 0.0, 0.125, 0.0, 0.0, 0.25, 0, 0, 0.875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.625, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.75, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0.25, 0.875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.375, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.625, 0.0, 0.0, 0.25, 0.0, 0.0, 0.375, 0.0, 0.0, 0.0, 0.625, 0.5, 0.0, 0.25, 0.0, 0.25, 0.0, 0, 0.25, 0.0, 0.25, 0.0, 0.0, 0.125, 0.0, 0.375, 0.25, 0.625, 0.0, 1.125, 0.0, 0.0, 0.0, 0.0, 0, 0.5, 0.0, 0.0, 0.625, 0.375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.125, 0.0, 0.0, 0.0, 0.0, 0.125, 0.25, 0.25, 0.625, 0, 0, 0.0, 0.125, 0.0, 0.0, 0.5, 0, 0.5, 0.0, 0.375, 0, 0.875, 0.0, 0.375, 0.75, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0.125, 0.375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.875, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 1.125, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0.125, 0.0, 0.125, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.125, 0.125, 0.0, 0.0, 0.125, 0.5, 0.0, 0.0, 0.25, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.75, 0.0, 0.0, 0.0, 0.0, 0.625, 0.0, 0, 0.0, 0.0, 1.0, 0.125, 1.625, 0.5, 0.125, 0.0, 0.375, 0.25, 0.0, 0.0, 0.25, 0.0, 0.75, 0.75, 0.0, 0.0, 0, 0.0, 0.5, 0, 0.0, 0.0, 1.125, 0.0, 0.0, 0.0, 0.0, 0.625, 0.0, 0.0, 0.25, 0.5, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 1.125, 0.375, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0, 0.375, 0.25, 0.0, 0.0, 0.125, 0.0, 0.25, 0.0, 0, 0.0, 0.0, 0.5, 0, 0.0, 0.25, 0.0, 0.375, 0.0, 0.625, 0.25, 0.0, 0.0, 0.0, 0.0, 0.5, 0.25, 0.125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.25, 0.125, 0.25, 0.0, 0.0, 0, 0.0, 0.125, 0.0, 0.25, 0.25, 0.0, 0.0, 0.0, 0.0, 0, 0.625, 0.0, 0.125, 0.0, 0.0, 0.125, 0.0, 0.125, 0.0, 1.25, 0.0, 0.0, 0.125, 0.0, 0.375, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0, 0.25, 0.25, 0, 0.0, 0.0, 0.25, 0.375, 0.625, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 1.25, 0, 0.125, 0.0, 0.0, 0.0, 0.375, 0, 0.0, 0.5, 0.0, 0.25, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 2.625, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.75, 0.25, 0.125, 0.0, 0.0, 0.125, 0.0, 0.625, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0, 0.125, 0.0, 0.0, 0.25, 0.25, 0.0, 0.0, 0.25, 0.0, 0.0, 0, 0.0, 0.625, 0.0, 0.0, 0.0, 0.375, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.625, 0.0, 0.625, 0.25, 0.375, 0.125, 0.0, 0.25, 0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.25, 0.0, 3.125, 0.0, 0.125, 0.0, 0.25, 0.75, 0.125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.625, 0.0, 0.0, 0.0, 0.0, 0.75, 0.125, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.625, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0.25, 0.0, 0.625, 0.25, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.25, 0.375, 0.25, 0.0, 0.0, 0.0, 0.125, 0.5, 0, 2.375, 1.375, 0.0, 0.0, 0.0, 0.0, 0.375, 0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.25, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.125, 0.125, 0, 0.0, 0.0, 0.125, 0.0, 0.625, 0.0, 0.125, 0, 0.0, 0, 0.0, 0.125, 0.125, 0.0, 0, 0.0, 0.0, 0, 0, 0.0, 0.375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.5, 0.0, 0.375, 0.375, 0.0, 0.25, 0.0, 0.125, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0, 0.125, 0.25, 0.0, 0.0, 0.0, 0.5, 0.0, 0.625, 0.25, 0.0, 0.0, 0.25, 0.0, 0.125, 0.0, 0.0, 0.375, 0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.625, 0.0, 0.625, 0.0, 0.0, 0.0, 0.5, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.375, 0.25, 0.0, 0.0, 0.0, 0.0, 0, 0.25, 0.375, 0.5, 0, 0.0, 0, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0.125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.375, 0, 0.0, 0.0, 0.0, 0, 0.25, 0.0, 0.0, 0.625, 0.875, 0.25, 0.0, 0.0, 0.0, 0.25, 0.25, 0.0, 0.75, 0.0, 1.75, 0.625, 0.0, 0.0, 0, 0.0, 0.125, 0, 0.0, 0.0, 0.625, 0.0, 0.0, 0.0, 0.0, 0.25, 0.375, 0.0, 0, 0.375, 0.25, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 1.875, 0.0, 0.0, 0.625, 0.625, 0.0, 0.0, 0.25, 0.0, 0.375, 0.25, 0.5, 0.0, 0.625, 0.0, 0.375, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.625, 0.875, 0.625, 0.625, 0, 0.0, 0.875, 0.0, 0.5, 0.5, 0.25, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.5, 0.0, 0.125, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.625, 0.125, 0.0, 0.0, 0.25, 0.0, 0.75, 0.875, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.25, 0.25, 0.25, 0, 0.0, 0.0, 0.0, 0.0, 0.125, 0.0, 0.375, 0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.25, 0.875, 0, 0, 0.0, 0.625, 0.125, 0.0, 1.875, 0.25, 0.625, 0.625, 1.125, 0.0, 0, 0.0, 0.25, 1.0, 0, 0.0, 0.125, 0.0, 0.0, 1.0, 0.0, 0.0, 0.625, 0.25, 0.625, 0.75, 0, 0, 0, 0.0, 0.25, 0.0, 0.625, 0.125, 0.0, 0.25, 0.0, 0.5, 0.0, 0, 0.0, 0.0, 1.375, 0.125, 0.0, 0.25, 0, 0.0, 0.125, 0.875, 0.0, 0.0, 0.0, 0.0, 0.5, 0, 0.5, 0.625, 0.0, 0.125, 0.125, 0.0, 0.25, 0.25, 0.875, 0.625, 0.375, 0.5, 0, 0.25, 0.0, 0.0, 0.0, 0.75, 0.25, 0.625, 0.875, 0.0, 0.625, 0.5, 0.0, 0.0, 0.25, 0.25, 0.625, 0.0, 0.0, 0.0, 0.0, 1.0, 1.25, 0.25, 0.0, 0.0, 0.25, 0, 0, 0.0, 0.0, 0, 0, 0.0, 0.0, 0.0, 0.625, 0.0, 0.875, 0.5, 0.125, 0.125, 0, 0.625, 0.0, 0.0, 0.0, 0, 0.0, 0, 0, 0.625, 0.0, 0.0, 0.0, 0.0, 0.875, 0.0, 0.0, 0.0, 0.375, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0.25, 0, 0.0, 0.125, 0.125, 0.0, 0.0, 0.0, 0.25, 0.625, 0.125, 0.0, 0.0, 0.125, 0.0, 0.0, 0.0, 0.0, 0.375, 0.0, 0.0, 0.625, 0.0, 0.5, 0.0, 0, 0.5, 0.0, 0.25, 0.0, 0.0, 0.0, 0.5, 0.875, 0.0, 0.0, 0.125, 0.0, 0.0, 0.0, 0.625, 0, 0.0, 0.0, 0, 0.125, 0, 0.375, 0.625, 0.0, 0.5, 0.0, 0.25, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0, 0.625, 0.375, 0.0, 0.375, 0.0, 0, 0.25, 1.0, 0.125, 0.0, 0, 0.25, 0, 0.5, 0, 0.125, 0.625, 0.0, 0.0, 0.625, 0.0, 0, 0.0, 0.625, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.25, 1.25, 1.25, 0.625, 0.0, 0.0, 0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.625, 0.0, 0.25, 0.0, 0, 0.875, 0.0, 0.125, 0.0, 0, 0.0, 0, 0.0, 0.125, 0.125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.25, 1.25, 0.0, 0.0, 0.0, 0.0, 0.125, 0.875, 0.375, 0.0, 0.0, 0.0, 0.0, 0.125, 0.75, 0.0, 0.0, 0.25, 0.5, 0.5, 0.25, 0.0, 0.0, 0.0, 0.5, 0.0, 0.125, 0.0, 0.0, 0.25, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.875, 0.25, 0.0, 0.375, 0.125, 0.0, 0.0, 0.5, 0.0, 0, 0.0, 0.125, 0.625, 0.75, 0, 1.125, 0.0, 0.75, 0.25, 0.5, 0, 0.0, 0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.125, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0, 0.0, 0.0, 0.375, 0.375, 0.0, 0.0, 0.875, 0.25, 0, 0.0, 0.0, 0.625, 0.0, 0.25, 0.0, 0, 0.0, 0.125, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.375, 0, 0.125, 0.0, 0.875, 0.25, 0.625, 1.25, 0.0, 0.375, 0.25, 0.125, 0.0, 0.0, 0.625, 0.0, 0.0, 0.0, 0.0, 1.125, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.875, 0.0, 0.625, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0, 0.0, 0.25, 0.0, 0.0, 0.125, 0.25, 0.25, 0.875, 3.75, 0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.75, 0, 0, 0.25, 0.625, 0.0, 0.0, 0, 0.875, 0.25, 0.0, 0.625, 0, 0.0, 1.0, 0, 4.375, 0.5, 0.0, 0.625, 0.125, 0.0, 0.875, 0.0, 0.0, 0.25, 0.0, 1.125, 0.0, 0.0, 0.0, 0.0, 0.0, 1.25, 0.5, 0.5, 0.0, 0.0, 1.125, 2.75, 0.625, 0.25, 1.25, 0.0, 0.0, 0.25, 0.0, 0.5, 0.0, 0.25, 0, 0.0, 0.0, 0.0]\n",
            "[0.25, 0.125, 0.0, 0, 0.0, 0, 0.125, 0.625, 0.0, 0.0, 0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.625, 0.0, 0.75, 0.5, 0.625, 0.0, 0.0, 0, 0.0, 0.125, 0.0, 0.0, 0.0, 0, 0.0, 0.125, 0.0, 0.0, 0.5, 0.125, 0.0, 0.125, 0.0, 0.25, 0.0, 0.0, 0.0, 0.25, 0.125, 0.0, 0.125, 0, 0.125, 0.375, 0.0, 0.0, 0.0, 0.875, 0.0, 0.125, 0.0, 0.25, 0.125, 0.0, 0.0, 0.125, 0.0, 0.0, 0.0, 0.125, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0.25, 0, 0.0, 0.0, 0, 0.0, 0.625, 0.0, 0.625, 0.0, 0.0, 0.125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.375, 0, 0.0, 0, 0.0, 1.0, 0.125, 0, 0.75, 0.0, 0.25, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.375, 0.0, 0.125, 0.5, 0.0, 0.5, 0.0, 0.0, 0.0, 0.125, 0.125, 0.0, 0.0, 0.0, 0.0, 0.125, 0.0, 0.0, 0.125, 0.0, 0.0, 0.0, 0.375, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0.25, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.125, 0.0, 0.0, 0.625, 0.125, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.5, 0.0, 0.125, 0.25, 0.0, 0.125, 0.0, 0.625, 0.125, 0.25, 0.0, 0.0, 0.25, 0.125, 0.25, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.25, 0.0, 0.125, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.875, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0.375, 0.125, 0.0, 0.0, 0.125, 0.0, 0.75, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.125, 0.0, 0.25, 0.0, 0.0, 0.125, 0.0, 0.0, 0.125, 0.25, 0.0, 0.25, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0.25, 0.0, 0.0, 0.25, 0.125, 0.0, 0.875, 0.625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.25, 0.25, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.375, 0.0, 0.0, 0.0, 0.5, 0.0, 0.25, 0.25, 0.0, 0, 0.25, 0.0, 0.0, 0.0, 0.25, 1.292, 0, 0.125, 0.0, 0.0, 0.125, 0.25, 0.25, 0.0, 0.0, 1.25, 0.125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.125, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0, 0.0, 0.0, 0.0, 0.625, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0.625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.5, 0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.75, 0.0, 0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.875, 0.0, 0.0, 0.0, 0.0, 0.625, 0.0, 0.0, 0.0, 0.0, 0.25, 0.125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.125, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.625, 0, 0.0, 0.0, 0.125, 0.0, 0.0, 0.5, 0.125, 0.0, 0.0, 0.125, 0.0, 0.0, 0.0, 0.0, 0.125, 0.625, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0.25, 0.25, 0.0, 0.75, 0, 0.0, 0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.625, 0.25, 0.25, 0.25, 0.0, 0.0, 0.0, 0.125, 0.25, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.125, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.625, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.25, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.125, 0.0, 0.0, 0.375, 0.75, 0, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.125, 0.0, 0, 0.0, 0.0, 0.5, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0.25, 0.0, 0, 0, 0.0, 0.0, 0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.125, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0, 0.75, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.125, 0.0, 0.0, 0.25, 0.0, 0.0, 0.625, 0.0, 0.375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.125, 0.25, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0.25, 0.25, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0.875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 1.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.125, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.25, 0.125, 0.0, 0.0, 0, 0.0, 0.0, 0.25, 0.0, 0.625, 0.0, 0.0, 0.0, 0.125, 0.625, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.75, 0.0, 0.0, 0.0, 0.125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.125, 0.0, 0.25, 0.0, 0.5, 0.0, 0.0, 0.0, 0, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.375, 0.0, 0.0, 0.0, 0.25, 0.25, 0.25, 0.0, 0.0, 0.25, 0.125, 0.0, 0, 0.25, 0, 0, 0.0, 0.25, 0.0, 0.0, 0.0, 0, 0.25, 0.0, 0.0, 0, 0.125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.125, 0.0, 0.75, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0.75, 0.0, 0.125, 0.25, 0.5, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.125, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.875, 0.0, 0, 0, 0.0, 0.0, 0.0, 0.375, 0.0, 0.25, 0.0, 0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.125, 0.0, 0.25, 0.0, 0, 1.0, 0.0, 0.25, 0.0, 0, 0.0, 0.25, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.5, 0.25, 0, 0.125, 0.0, 0.0, 0.25, 0, 0.25, 0.0, 0.0, 0.0, 0.125, 0.75, 0.0, 0.375, 0.0, 0.0, 0.0, 0.375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.25, 0.0, 0.0, 0, 0.0, 0.0, 0.125, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.25, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0.375, 0.25, 0.0, 0.0, 0.0, 0.0, 0.125, 0.25, 0.5, 0.0, 0.375, 0.0, 0.125, 0.0, 0.0, 0.0, 0.0, 0.875, 0.125, 0.125, 0.0, 0.0, 0.0, 0.0, 0.125, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0.375, 0.0, 0.0, 0.0, 0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.125, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.375, 0.125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.125, 0.0, 0.0, 0, 0.0, 0.0, 0.625, 0.0, 0.125, 0.0, 0.75, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.5, 0.0, 0.0, 0, 0.125, 0.0, 0.0, 0.0, 0.375, 0.5, 0.0, 0.375, 0.0, 0.25, 0, 0.0, 0, 0.125, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0, 0.0, 0.125, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0.25, 0.0, 0.0, 0.25, 0.0, 0.0, 0, 1.125, 0.0, 0.0, 0, 0.0, 0.0, 0, 0.0, 0.0, 0.5, 0.0, 0.125, 0, 0, 0.0, 0, 0.75, 0.5, 0.0, 0, 0.0, 0.0, 0.0, 0, 0.25, 0.25, 0.75, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.125, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.125, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.375, 0.25, 0.0, 0.375, 0.0, 0.125, 0.5, 0.0, 0.0, 1.25, 0.0, 0.0, 0.0, 0.125, 0, 0.125, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 1.125, 0.0, 0.0, 0.625, 0.0, 0.0, 0.875, 0.0, 0.5, 0.0, 0.375, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.875, 0.875, 0.0, 0.0, 0, 0.125, 0, 0.0, 0.0, 0.625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.125, 0, 0.0, 0.25, 0.5, 0.25, 0.0, 0.0, 0.0, 0, 0.0, 0, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.375, 0.0, 0.25, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.125, 0.5, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0, 0.0, 0, 0, 0.0, 0.0, 0.0, 0.25, 0.5, 0.0, 0.0, 0.0, 0.0, 0.125, 0.0, 0.0, 0.0, 0.125, 0.0, 0, 0.0, 0.0, 0.875, 0.0, 0.0, 0.125, 0.0, 1.5, 0.0, 0.0, 0.0, 0, 0.125, 0.0, 0.25, 0.0, 0.25, 0.125, 0.375, 0.125, 0.0, 0.0, 0.0, 0.125, 0.0, 0.0, 0.875, 0.125, 0.0, 0.0, 0.0, 0.125, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.875, 0.75, 0.0, 0.125, 0.0, 0.0, 0.75, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.125, 0.25, 0, 0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.25, 0.0, 0, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.25, 0.0, 0.125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0, 0.0, 0.0, 0.375, 0, 0.5, 0.0, 0.0, 0.375, 0.0, 0.0, 0.0, 0.0, 0.375, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.125, 0.0, 0.125, 0.625, 0.0, 0.875, 0.0, 0.0, 0.0, 0.0, 0, 0.5, 0.0, 0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.5, 0.0, 0.125, 0.125, 0.5, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0, 0.0, 0.375, 0.125, 0.25, 0.0, 0.125, 0.0, 0.625, 0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.25, 0.125, 0.625, 0.0, 0.125, 0.125, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0, 0.0, 0.0, 0.125, 0.25, 0, 0.0, 0.0, 0.0, 0.0, 0.375, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0, 0.25, 0.0, 0.0, 0.5, 0.75, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.125, 0.0, 0.25, 1.125, 0.0, 0.25, 0, 0.0, 0.0, 0.375, 0.0, 0.0, 0.125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.75, 0.125, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.125, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.625, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0.125, 0.25, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0, 0.125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.125, 0.0, 0.5, 0.75, 0.0, 0.0, 0.0, 0.0, 0.875, 0.125, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0, 0.0, 0.125, 0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.125, 0.0, 0, 0.0, 0.0, 1.125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.125, 0.0, 0.375, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0, 0.125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.375, 0, 0, 0.0, 0.25, 0.0, 0.375, 0.0, 0.25, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0, 0.375, 0.0, 0, 0.0, 0.375, 0.0, 0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.125, 0.25, 0.0, 0, 0.0, 0.0, 0.25, 0.0, 0.0, 0, 0, 0.0, 0.0, 0, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.625, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.625, 0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.125, 0.0, 0.0, 0.0, 0.125, 0.0, 0.25, 0.0, 0.0, 0.375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.375, 0.0, 0.0, 0.0, 0.625, 0.0, 0, 0.0, 0, 0, 0.625, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.625, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.875, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.125, 0.0, 0.25, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0, 0.0, 0, 0, 0, 0.0, 1.0, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.125, 0.0, 0.0, 0.0, 0, 0.0, 0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.25, 0.0, 0.0, 0.0, 0, 0.0, 0.25, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.125, 0.0, 0.0, 0.375, 0.25, 0.0, 0.0, 0.0, 0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.125, 0.0, 0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0, 0, 0.0, 1.75, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.125, 0.0, 0, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0, 0.0, 0.0, 0.0, 0.375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.375, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0, 0.25, 0, 0, 0.25, 0.25, 0.375, 0.25, 0, 0.0, 0.0, 0.25, 0.625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.75, 0.0, 0.0, 0.0, 0.0, 0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0, 0.0, 0.0, 0.0, 0.625, 0.0, 0.0, 0.875, 0, 0.0, 0.0, 0.125, 0.0, 0, 0.0, 0.0, 0, 0.0, 0.0, 0.125, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0.625, 0.0, 0.125, 0.0, 0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0.625, 0.25, 0.0, 0.0, 0.0, 0, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.125, 0.25, 0.0, 0.125, 0.0, 0.0, 0.125, 0.0, 0.0, 0.0, 0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.25, 0.0, 0.375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.25, 0.25, 0.0, 0.0, 0.25, 0.125, 0.25, 0.0, 0.0, 0, 0.0, 0.0, 0.125, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.375, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.25, 0, 0.125, 0.0, 0.0, 0.625, 0.125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0, 0.375, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.25, 0.0, 0.0, 0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.75, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.125, 0.25, 0.0, 0.0, 0.25, 0.0, 1.75, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0, 0.625, 0.0, 0.125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.125, 0.0, 0.0, 0.375, 0.125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0.125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.75, 0.0, 0.0, 0.125, 0.25, 0.0, 0.0, 0.25, 0.0, 0.25, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0, 0, 0.0, 0.0, 0.25, 0.25, 0.875, 0, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0.75, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.25, 0.25, 0.0, 0, 0.0, 0.75, 0.375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.125, 0.0, 0.375, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0.5, 0, 0.0, 0.0, 0.0, 0.0, 0.25, 0, 0.375, 0.0, 0.0, 0.0, 0.25, 0.25, 0.25, 0.125, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.125, 0.0, 0.0, 0.0, 0.125, 0, 0.0, 0.0, 0.75, 0.0, 0.0, 0.5, 0.5, 0.0, 0.0, 0.0, 0.375, 0.0, 0.375, 0.0, 0.0, 0.0, 0.0, 0.375, 0, 0.0, 0.375, 0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.375, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.125, 1.5, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0.125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0, 0.0, 0.25, 0.0, 0.25, 0.0, 0.375, 0.0, 0.25, 0.375, 0.0, 0.0, 0.0, 0.25, 0.375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0.25, 0.0, 0.125, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0.25, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.75, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0.125, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.125, 0.375, 0, 0.0, 0.0, 0.375, 0.0, 0.125, 0.0, 0.5, 0.0, 0, 0.0, 0.0, 0.125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.75, 0.0, 0.0, 0.0, 0.5, 0, 0.0, 0.625, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0.25, 0.0, 0.0, 0.125, 0.0, 0.375, 0.0, 0.0, 0.0, 0, 0.375, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.125, 0.0, 0.0, 0.0, 0.0, 1.25, 0.0, 0.0, 0, 0.0, 0.5, 0.625, 0.5, 0.0, 0.25, 0.25, 0.0, 0.25, 0.0, 0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.75, 0.0, 0.375, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0, 0.0, 0.0, 0.0, 1.25, 0.0, 0.0, 0.0, 0.0, 0.125, 0.0, 0.125, 0.0, 0.25, 0.0, 0.625, 0.0, 0.25, 0.0, 0.0, 0.375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.25, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0.125, 0.0, 0.25, 0.0, 0, 0.125, 0.0, 0.125, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.125, 0.125, 0.0, 0.0, 0.0, 0.0, 0.125, 0, 0.0, 0.0, 0.0, 0.125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.5, 0.0, 0, 0.0, 0.0, 0.5, 0.25, 0.125, 0.0, 0.125, 0, 0.0, 0, 0.25, 0.5, 0.0, 0.0, 0, 0.0, 0.0, 0, 0, 0.0, 0.5, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.625, 0.125, 0.0, 0.375, 0.0, 0.5, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.25, 0, 0, 0.5, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.625, 0.0, 0.25, 0.5, 0.0, 0.0, 0.25, 0, 0.0, 0.0, 0.125, 0.0, 0.0, 0.75, 0.0, 0.625, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.625, 0.0, 0.125, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0, 0.0, 0, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0.25, 0.0, 0.125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.25, 0, 0.25, 0.125, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.625, 0.0, 0, 0.0, 0.125, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.875, 0.0, 0, 0.0, 0.0, 0.25, 0.0, 1.125, 0.0, 0.0, 0.0, 0.0, 0.625, 0.0, 0.0, 0.0, 0.375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0.125, 0.125, 0.125, 0.5, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.875, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.375, 0.0, 0.125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.875, 0.0, 0.0, 0.0, 0.25, 0.0, 0.25, 0.25, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.625, 0.0, 0.25, 0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.5, 0.0, 0.25, 0.0, 0.5, 0.25, 0.125, 0, 0, 0.0, 1.0, 0.125, 0.0, 0.0, 0.0, 0.125, 0.0, 0.0, 0.0, 0, 0.5, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.375, 0.0, 0.0, 0.125, 0, 0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.25, 0.0, 0, 0.0, 0.0, 0.875, 0.75, 0.0, 0.0, 0, 0.0, 0.75, 0.0, 0.0, 0.0, 0.0, 0.125, 0.0, 0, 0.0, 0.0, 0.0, 1.625, 0.0, 0.0, 0.0, 0.0, 0.125, 0.5, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.125, 0.0, 0.0, 0.125, 0, 0, 0.0, 0.0, 0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.75, 0.125, 0, 0.0, 0.0, 0.0, 0.25, 0, 0.0, 0, 0, 0.125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.375, 0.0, 0.125, 0.0, 0.25, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0.25, 0, 0.25, 0.25, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.375, 0.375, 0.25, 0.125, 0.0, 0.0, 0, 0.0, 0.75, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0, 0.0, 0, 0.625, 0.0, 0.875, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.125, 0.0, 0.0, 0.0, 0, 0.375, 0, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0.25, 0, 0.0, 0.25, 0.0, 0.0, 0.25, 0.0, 0.0, 0.125, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.125, 0.375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.125, 0.0, 0, 0.0, 0.0, 0.375, 0.0, 0, 0.0, 0, 0.0, 0.0, 0.125, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.125, 0.25, 0.0, 0.0, 0.125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0, 0.125, 0.0, 0.25, 0.125, 0.0, 0, 0.0, 0, 0.0, 0.25, 0.125, 0.0, 0.125, 0.125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.125, 0.0, 0.25, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.75, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0, 0.0, 0.125, 0.0, 0.0, 0.0, 0.25, 0.0, 0.125, 0.0, 0, 0.625, 0.0, 0.375, 0.0, 0.0, 0.75, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.125, 0, 0, 0.75, 0.0, 0.0, 0.0, 0, 0.0, 0.125, 0.0, 0.125, 0, 0.25, 0.0, 0, 0.625, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.25, 0.0, 0.0, 0.125, 0.0, 0.0, 0.375, 0, 0.0, 0.0, 0.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['Capital_Words_Count']=capital_words_count\n",
        "df['Extended_Words_Count']= extended_words_count\n",
        "df['Exclamation_at_End']= exclamation_at_end\n",
        "df['Repeated_Punctuation_Count']= repeated_punctuation_count\n",
        "df['Sentiment_Scores_positive']= pos_sentiment_score\n",
        "df['Sentiment_Scores_negetive']= neg_sentiment_score"
      ],
      "metadata": {
        "id": "K9RPza_-pqJQ"
      },
      "id": "K9RPza_-pqJQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "id": "4fojrNHDpqCV",
        "outputId": "1e9bd9cf-049d-4390-b7ac-bb468ac6be7d"
      },
      "id": "4fojrNHDpqCV",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text  category  \\\n",
              "0  Ye song nahi hi Ye MODI Ji ka mehnat ka rang h...  Positive   \n",
              "1  Love u sir love u soo much urs I'ts beautyful ...  Positive   \n",
              "2  Arae sur jee pahelae hamare bharat ke bachho k...   Neutral   \n",
              "3  Wah! Jitni sundar geet ke bhao hain utnihi sun...  Positive   \n",
              "4  Sundar ekdam sahi Gaya Hua gana.chhotisi gudiy...  Positive   \n",
              "\n",
              "   Capital_Words_Count  Extended_Words_Count  Exclamation_at_End  \\\n",
              "0                    1                     0                   0   \n",
              "1                    1                     1                   0   \n",
              "2                    0                     4                   0   \n",
              "3                    0                     3                   0   \n",
              "4                    0                     1                   0   \n",
              "\n",
              "   Repeated_Punctuation_Count  Sentiment_Scores_positive  \\\n",
              "0                           1                      0.000   \n",
              "1                           1                      1.375   \n",
              "2                           1                      0.000   \n",
              "3                           0                      0.000   \n",
              "4                           0                      0.000   \n",
              "\n",
              "   Sentiment_Scores_negetive  \n",
              "0                      0.250  \n",
              "1                      0.125  \n",
              "2                      0.000  \n",
              "3                      0.000  \n",
              "4                      0.000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5eb3d808-4b2d-4244-8616-5a4043b8950a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>category</th>\n",
              "      <th>Capital_Words_Count</th>\n",
              "      <th>Extended_Words_Count</th>\n",
              "      <th>Exclamation_at_End</th>\n",
              "      <th>Repeated_Punctuation_Count</th>\n",
              "      <th>Sentiment_Scores_positive</th>\n",
              "      <th>Sentiment_Scores_negetive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Ye song nahi hi Ye MODI Ji ka mehnat ka rang h...</td>\n",
              "      <td>Positive</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Love u sir love u soo much urs I'ts beautyful ...</td>\n",
              "      <td>Positive</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.375</td>\n",
              "      <td>0.125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Arae sur jee pahelae hamare bharat ke bachho k...</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Wah! Jitni sundar geet ke bhao hain utnihi sun...</td>\n",
              "      <td>Positive</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Sundar ekdam sahi Gaya Hua gana.chhotisi gudiy...</td>\n",
              "      <td>Positive</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5eb3d808-4b2d-4244-8616-5a4043b8950a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-5eb3d808-4b2d-4244-8616-5a4043b8950a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-5eb3d808-4b2d-4244-8616-5a4043b8950a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-015ec08f-7908-44a9-ade2-0cbb70463921\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-015ec08f-7908-44a9-ade2-0cbb70463921')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-015ec08f-7908-44a9-ade2-0cbb70463921 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "for train_index, test_index in kf.split(df.text):\n",
        "    break\n",
        "\n",
        "test_df = df.iloc[test_index]\n",
        "kf2 = KFold(n_splits=10, shuffle=True, random_state=42)\n",
        "for train_index, val_index in kf2.split(df.iloc[train_index].text):\n",
        "    break\n",
        "\n",
        "val_df = df.iloc[val_index]\n",
        "train_df = df.iloc[train_index]"
      ],
      "metadata": {
        "id": "9eXcjaueN3Fn"
      },
      "id": "9eXcjaueN3Fn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.shape, val_df.shape, test_df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrk4RLTfNkPg",
        "outputId": "72ff0ede-12df-498f-9229-c5488bd3fbbc"
      },
      "id": "wrk4RLTfNkPg",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((2792, 8), (311, 8), (776, 8))"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1e2de8d",
      "metadata": {
        "id": "e1e2de8d"
      },
      "outputs": [],
      "source": [
        "def clean_tweets(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'@\\w+','',text)\n",
        "    text = re.sub(r'http\\w+','',text)\n",
        "    text = re.sub(r'#\\w+','',text)\n",
        "    text = re.sub(r'\\d+','',text)\n",
        "    return text.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8db0f93d",
      "metadata": {
        "id": "8db0f93d",
        "outputId": "8f4154f8-4337-4b40-f3e8-b4f82caf27c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text  category  \\\n",
            "1  love u sir love u soo much urs i'ts beautyful ...  Positive   \n",
            "2  arae sur jee pahelae hamare bharat ke bachho k...   Neutral   \n",
            "3  wah! jitni sundar geet ke bhao hain utnihi sun...  Positive   \n",
            "4  sundar ekdam sahi gaya hua gana.chhotisi gudiy...  Positive   \n",
            "5                                  wao lata mangekar  Positive   \n",
            "\n",
            "   Capital_Words_Count  Extended_Words_Count  Exclamation_at_End  \\\n",
            "1                    1                     1                   0   \n",
            "2                    0                     4                   0   \n",
            "3                    0                     3                   0   \n",
            "4                    0                     1                   0   \n",
            "5                    0                     0                   0   \n",
            "\n",
            "   Repeated_Punctuation_Count  Sentiment_Scores_positive  \\\n",
            "1                           1                      1.375   \n",
            "2                           1                      0.000   \n",
            "3                           0                      0.000   \n",
            "4                           0                      0.000   \n",
            "5                           0                      0.000   \n",
            "\n",
            "   Sentiment_Scores_negetive  \n",
            "1                      0.125  \n",
            "2                      0.000  \n",
            "3                      0.000  \n",
            "4                      0.000  \n",
            "5                      0.000  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-bce624efedff>:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_df.text = train_df.text.apply(lambda x: clean_tweets(x))\n",
            "<ipython-input-19-bce624efedff>:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  val_df.text = val_df.text.apply(lambda x: clean_tweets(x))\n",
            "<ipython-input-19-bce624efedff>:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  test_df.text = test_df.text.apply(lambda x: clean_tweets(x))\n"
          ]
        }
      ],
      "source": [
        "train_df.text = train_df.text.apply(lambda x: clean_tweets(x))\n",
        "val_df.text = val_df.text.apply(lambda x: clean_tweets(x))\n",
        "test_df.text = test_df.text.apply(lambda x: clean_tweets(x))\n",
        "\n",
        "print(train_df[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f51c2e0",
      "metadata": {
        "id": "4f51c2e0",
        "outputId": "0e746bca-81c8-4620-aa3f-036b6ac84ded",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2792 311 776\n"
          ]
        }
      ],
      "source": [
        "print(len(train_df['text']), len(val_df['text']), len(test_df['text']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8fc68ac",
      "metadata": {
        "id": "c8fc68ac"
      },
      "outputs": [],
      "source": [
        "def _get_unique(elems):\n",
        "    if type(elems[0]) == list:\n",
        "        corpus = flatten(elems)\n",
        "    else:\n",
        "        corpus = elems\n",
        "    elems, freqs = zip(*Counter(corpus).most_common())\n",
        "    return list(elems)\n",
        "\n",
        "\n",
        "def convert_categorical_label_to_int(labels):\n",
        "    if type(labels[0]) == list:\n",
        "        uniq_labels = _get_unique(flatten(labels))\n",
        "    else:\n",
        "        uniq_labels = _get_unique(labels)\n",
        "\n",
        "\n",
        "    if type(labels[0]) == list:\n",
        "        label_to_id = {w:i+1 for i,w in enumerate(uniq_labels)}\n",
        "    else:\n",
        "        label_to_id = {w:i for i,w in enumerate(uniq_labels)}\n",
        "\n",
        "    new_labels = []\n",
        "    if type(labels[0]) == list:\n",
        "        for i in labels:\n",
        "            new_labels.append([label_to_id[j] for j in i])\n",
        "    else:\n",
        "        new_labels = [label_to_id[j] for j in labels]\n",
        "\n",
        "    return new_labels, label_to_id"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        },
        "id": "npztD-zHSe5o",
        "outputId": "ffc3fb7e-0c33-4126-a13a-eb9bd53d03d2"
      },
      "id": "npztD-zHSe5o",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text  category  \\\n",
              "1  love u sir love u soo much urs i'ts beautyful ...  Positive   \n",
              "2  arae sur jee pahelae hamare bharat ke bachho k...   Neutral   \n",
              "3  wah! jitni sundar geet ke bhao hain utnihi sun...  Positive   \n",
              "4  sundar ekdam sahi gaya hua gana.chhotisi gudiy...  Positive   \n",
              "5                                  wao lata mangekar  Positive   \n",
              "\n",
              "   Capital_Words_Count  Extended_Words_Count  Exclamation_at_End  \\\n",
              "1                    1                     1                   0   \n",
              "2                    0                     4                   0   \n",
              "3                    0                     3                   0   \n",
              "4                    0                     1                   0   \n",
              "5                    0                     0                   0   \n",
              "\n",
              "   Repeated_Punctuation_Count  Sentiment_Scores_positive  \\\n",
              "1                           1                      1.375   \n",
              "2                           1                      0.000   \n",
              "3                           0                      0.000   \n",
              "4                           0                      0.000   \n",
              "5                           0                      0.000   \n",
              "\n",
              "   Sentiment_Scores_negetive  \n",
              "1                      0.125  \n",
              "2                      0.000  \n",
              "3                      0.000  \n",
              "4                      0.000  \n",
              "5                      0.000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d6c6d462-172b-4ff2-a89e-9efc3effba09\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>category</th>\n",
              "      <th>Capital_Words_Count</th>\n",
              "      <th>Extended_Words_Count</th>\n",
              "      <th>Exclamation_at_End</th>\n",
              "      <th>Repeated_Punctuation_Count</th>\n",
              "      <th>Sentiment_Scores_positive</th>\n",
              "      <th>Sentiment_Scores_negetive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>love u sir love u soo much urs i'ts beautyful ...</td>\n",
              "      <td>Positive</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.375</td>\n",
              "      <td>0.125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>arae sur jee pahelae hamare bharat ke bachho k...</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>wah! jitni sundar geet ke bhao hain utnihi sun...</td>\n",
              "      <td>Positive</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>sundar ekdam sahi gaya hua gana.chhotisi gudiy...</td>\n",
              "      <td>Positive</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>wao lata mangekar</td>\n",
              "      <td>Positive</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d6c6d462-172b-4ff2-a89e-9efc3effba09')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d6c6d462-172b-4ff2-a89e-9efc3effba09 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d6c6d462-172b-4ff2-a89e-9efc3effba09');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-863f3bd1-17d5-4a6f-9e40-cc86daf2dd7e\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-863f3bd1-17d5-4a6f-9e40-cc86daf2dd7e')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-863f3bd1-17d5-4a6f-9e40-cc86daf2dd7e button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b61d9b78",
      "metadata": {
        "id": "b61d9b78",
        "outputId": "be1673a7-cf2a-4ef8-ce62-3c3a1b9e7275",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-23-930380e1caab>:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_df.category, label2idx = convert_categorical_label_to_int(train_df.category.values)\n"
          ]
        }
      ],
      "source": [
        "train_df.category, label2idx = convert_categorical_label_to_int(train_df.category.values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "956fbde6",
      "metadata": {
        "id": "956fbde6",
        "outputId": "1abdcfed-1f47-453f-9755-8c8682842185",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Neutral': 0, 'Positive': 1, 'Negative': 2}\n",
            "1     1\n",
            "2     0\n",
            "3     1\n",
            "4     1\n",
            "5     1\n",
            "6     1\n",
            "7     1\n",
            "8     1\n",
            "9     2\n",
            "10    0\n",
            "Name: category, dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-24-7a26b4210ef7>:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  val_df.category, _ = convert_categorical_label_to_int(val_df.category.values)\n",
            "<ipython-input-24-7a26b4210ef7>:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  test_df.category, _ = convert_categorical_label_to_int(test_df.category.values)\n"
          ]
        }
      ],
      "source": [
        "val_df.category, _ = convert_categorical_label_to_int(val_df.category.values)\n",
        "test_df.category, _ = convert_categorical_label_to_int(test_df.category.values)\n",
        "\n",
        "print(label2idx)\n",
        "\n",
        "print(train_df.category[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7432989",
      "metadata": {
        "id": "f7432989",
        "outputId": "3ae15a39-ca41-4212-ef4e-c9eaee612daf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: 'Neutral', 1: 'Positive', 2: 'Negative'}\n"
          ]
        }
      ],
      "source": [
        "idx2label = {i:w for (w, i) in label2idx.items()}\n",
        "print(idx2label)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bangla Emb"
      ],
      "metadata": {
        "id": "nlCZsvIDb2_4"
      },
      "id": "nlCZsvIDb2_4"
    },
    {
      "cell_type": "code",
      "source": [
        "vocab,embeddings = [],[]\n",
        "with open('/content/drive/MyDrive/nlp_project/custom_embedding.txt','rt') as fi:\n",
        "    full_content = fi.read().strip().split('\\n')\n",
        "for i in range(len(full_content)):\n",
        "    i_word = full_content[i].split(' ')[0]\n",
        "    i_embeddings = [float(val) for val in full_content[i].split(' ')[1:]]\n",
        "    vocab.append(i_word)\n",
        "    embeddings.append(i_embeddings)"
      ],
      "metadata": {
        "id": "70ne4a-FHq11"
      },
      "execution_count": null,
      "outputs": [],
      "id": "70ne4a-FHq11"
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_npa = np.array(vocab)\n",
        "embs_npa = np.array(embeddings)"
      ],
      "metadata": {
        "id": "IznVCK83LQkw"
      },
      "execution_count": null,
      "outputs": [],
      "id": "IznVCK83LQkw"
    },
    {
      "cell_type": "code",
      "source": [
        "len(vocab_npa), print(vocab_npa[:10])"
      ],
      "metadata": {
        "id": "hb4b3HQkds9h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74611a59-5627-44fd-c815-5669223071c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<unk>' '<s>' '</s>' 's' '▁the' '▁' '▁to' 'e' '▁i' '▁you']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7000, None)"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ],
      "id": "hb4b3HQkds9h"
    },
    {
      "cell_type": "code",
      "source": [
        "embs_npa.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zh7RR5a75cO7",
        "outputId": "abd4e2e3-f8c5-4e8c-acef-038f801797c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7000, 128)"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ],
      "id": "Zh7RR5a75cO7"
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_npa = np.insert(vocab_npa, 0, '<pad>')\n",
        "# vocab_npa = np.insert(vocab_npa, 1, '[UNK]')\n",
        "print(vocab_npa[:10])\n",
        "\n",
        "pad_emb_npa = np.zeros((1,embs_npa.shape[1]))   #embedding for '<pad>' token.\n",
        "# unk_emb_npa = np.mean(embs_npa,axis=0,keepdims=True)    #embedding for '<unk>' token.\n",
        "\n",
        "#insert embeddings for pad and unk tokens at top of embs_npa.\n",
        "# embs_npa = np.vstack((pad_emb_npa,unk_emb_npa,embs_npa))\n",
        "embs_npa = np.vstack((pad_emb_npa,embs_npa))\n",
        "print(embs_npa.shape)"
      ],
      "metadata": {
        "id": "51wUtjf7LYJx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a1ad05d-d67a-470a-f419-b2fb1846580e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<pad>' '<unk>' '<s>' '</s>' 's' '▁the' '▁' '▁to' 'e' '▁i']\n",
            "(7001, 128)\n"
          ]
        }
      ],
      "id": "51wUtjf7LYJx"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Continue"
      ],
      "metadata": {
        "id": "UuPCzC78kRm9"
      },
      "id": "UuPCzC78kRm9"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fasttext"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kr2_i7_LVp1k",
        "outputId": "c3362986-02ee-4e22-f635-8a83ed188183"
      },
      "id": "Kr2_i7_LVp1k",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/68.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.8/68.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pybind11>=2.2 (from fasttext)\n",
            "  Using cached pybind11-2.11.1-py3-none-any.whl (227 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext) (67.7.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fasttext) (1.23.5)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp310-cp310-linux_x86_64.whl size=4199772 sha256=6a231aed45204ca23ca4d90a7d9d72942b308bc97c44ff824934e5293db6df75\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/13/75/f811c84a8ab36eedbaef977a6a58a98990e8e0f1967f98f394\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.2 pybind11-2.11.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fasttext"
      ],
      "metadata": {
        "id": "x2ll81LTVoPc"
      },
      "id": "x2ll81LTVoPc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.text = df.text.apply(lambda x: clean_tweets(x))\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "id": "HUXT-XCjYXZ1",
        "outputId": "855fa2d7-674c-430d-873d-a7e243cb59ae"
      },
      "id": "HUXT-XCjYXZ1",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text  category  \\\n",
              "0  ye song nahi hi ye modi ji ka mehnat ka rang h...  Positive   \n",
              "1  love u sir love u soo much urs i'ts beautyful ...  Positive   \n",
              "2  arae sur jee pahelae hamare bharat ke bachho k...   Neutral   \n",
              "3  wah! jitni sundar geet ke bhao hain utnihi sun...  Positive   \n",
              "4  sundar ekdam sahi gaya hua gana.chhotisi gudiy...  Positive   \n",
              "\n",
              "   Capital_Words_Count  Extended_Words_Count  Exclamation_at_End  \\\n",
              "0                    1                     0                   0   \n",
              "1                    1                     1                   0   \n",
              "2                    0                     4                   0   \n",
              "3                    0                     3                   0   \n",
              "4                    0                     1                   0   \n",
              "\n",
              "   Repeated_Punctuation_Count  Sentiment_Scores_positive  \\\n",
              "0                           1                      0.000   \n",
              "1                           1                      1.375   \n",
              "2                           1                      0.000   \n",
              "3                           0                      0.000   \n",
              "4                           0                      0.000   \n",
              "\n",
              "   Sentiment_Scores_negetive  \n",
              "0                      0.250  \n",
              "1                      0.125  \n",
              "2                      0.000  \n",
              "3                      0.000  \n",
              "4                      0.000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7050c5d6-3eac-44d3-8432-f09859c6730a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>category</th>\n",
              "      <th>Capital_Words_Count</th>\n",
              "      <th>Extended_Words_Count</th>\n",
              "      <th>Exclamation_at_End</th>\n",
              "      <th>Repeated_Punctuation_Count</th>\n",
              "      <th>Sentiment_Scores_positive</th>\n",
              "      <th>Sentiment_Scores_negetive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ye song nahi hi ye modi ji ka mehnat ka rang h...</td>\n",
              "      <td>Positive</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>love u sir love u soo much urs i'ts beautyful ...</td>\n",
              "      <td>Positive</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.375</td>\n",
              "      <td>0.125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>arae sur jee pahelae hamare bharat ke bachho k...</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>wah! jitni sundar geet ke bhao hain utnihi sun...</td>\n",
              "      <td>Positive</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>sundar ekdam sahi gaya hua gana.chhotisi gudiy...</td>\n",
              "      <td>Positive</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7050c5d6-3eac-44d3-8432-f09859c6730a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7050c5d6-3eac-44d3-8432-f09859c6730a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7050c5d6-3eac-44d3-8432-f09859c6730a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-6b35a837-89b9-4486-9819-005d4c77535e\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6b35a837-89b9-4486-9819-005d4c77535e')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-6b35a837-89b9-4486-9819-005d4c77535e button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.text.to_csv(r'data.txt', header=None, index=None, sep=' ', mode='a')"
      ],
      "metadata": {
        "id": "77m71xdXYOE6"
      },
      "id": "77m71xdXYOE6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# t = \"__label__\"+df.category.str.lower()+\" \"+df.text\n",
        "# t.to_csv(r'supervised.txt', header=None, index=None, sep=' ', mode='a')"
      ],
      "metadata": {
        "id": "xEGpsxMNcLrz"
      },
      "id": "xEGpsxMNcLrz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fastText_model = fasttext.train_unsupervised('data.txt', model='skipgram', dim=300, minn=2, maxn=10)"
      ],
      "metadata": {
        "id": "Q2gSk1zOWtIH"
      },
      "id": "Q2gSk1zOWtIH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fastText_model['the'].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tP4vSpfvWtEk",
        "outputId": "6069648e-58a0-4884-dd20-a004d3085dbf"
      },
      "id": "tP4vSpfvWtEk",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(300,)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model = fasttext.train_supervised('supervised.txt')"
      ],
      "metadata": {
        "id": "WCC3S8egWtAr"
      },
      "id": "WCC3S8egWtAr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model.predict(\"Which baking dish is best to bake a banana bread ?\")"
      ],
      "metadata": {
        "id": "kwQlybOJWs9L"
      },
      "id": "kwQlybOJWs9L",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fastText_model.get_input_matrix().shape"
      ],
      "metadata": {
        "id": "lC_LKv2lWr2x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bfbc94f-c8f8-4592-ec62-d474860d9bb4"
      },
      "id": "lC_LKv2lWr2x",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2001189, 300)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72db9a0b",
      "metadata": {
        "id": "72db9a0b",
        "outputId": "154a1338-4257-43c5-e4a2-ac77fb9adee2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-1ad1e151da64>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0membedding_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mfastText_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFastText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membedding_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mfastText_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fastText_model.model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/fasttext.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, corpus_file, sg, hs, vector_size, alpha, window, min_count, max_vocab_size, word_ngrams, sample, seed, workers, min_alpha, negative, ns_exponent, cbow_mean, hashfxn, epochs, null_word, min_n, max_n, sorted_vocab, bucket, trim_rule, batch_words, callbacks, max_final_vocab, shrink_windows)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors_ngrams_lockf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mREAL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         super(FastText, self).__init__(\n\u001b[0m\u001b[1;32m    436\u001b[0m             \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvector_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, corpus_file, vector_size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, epochs, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, comment, max_final_vocab, shrink_windows)\u001b[0m\n\u001b[1;32m    427\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcorpus_iterable\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcorpus_file\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_corpus_sanity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m             self.train(\n\u001b[1;32m    431\u001b[0m                 \u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mbuild_vocab\u001b[0;34m(self, corpus_iterable, corpus_file, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)\u001b[0m\n\u001b[1;32m    495\u001b[0m         \u001b[0mreport_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_raw_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeep_raw_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0mreport_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'memory'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimate_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreport_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_retained_words'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    498\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_lifecycle_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"build_vocab\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mprepare_weights\u001b[0;34m(self, update)\u001b[0m\n\u001b[1;32m    851\u001b[0m         \u001b[0;31m# set initial input/projection and hidden weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 853\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    854\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36minit_weights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    862\u001b[0m         \u001b[0;34m\"\"\"Reset all projection weights to an initial (untrained) state, but keep the existing vocabulary.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"resetting layer weights\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 864\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/fasttext.py\u001b[0m in \u001b[0;36mresize_vectors\u001b[0;34m(self, seed)\u001b[0m\n\u001b[1;32m   1154\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors_vocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprep_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprior_vectors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1155\u001b[0m         \u001b[0mngrams_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbucket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1156\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors_ngrams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprep_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngrams_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprior_vectors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors_ngrams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallocate_vecattrs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mprep_vectors\u001b[0;34m(target_shape, prior_vectors, seed, dtype)\u001b[0m\n\u001b[1;32m   2113\u001b[0m     \u001b[0mtarget_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2114\u001b[0m     \u001b[0mrng\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_rng\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# use new instance of numpy's recommended generator/algorithm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2115\u001b[0;31m     \u001b[0mnew_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrng\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [0.0, 1.0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2116\u001b[0m     \u001b[0mnew_vectors\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0;36m2.0\u001b[0m  \u001b[0;31m# [0.0, 2.0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2117\u001b[0m     \u001b[0mnew_vectors\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1.0\u001b[0m  \u001b[0;31m# [-1.0, 1.0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from gensim.models import FastText\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "\n",
        "fastText_model = None\n",
        "#if not os.path.exists('fastText_model.model'):\n",
        "tokenized = [simple_preprocess(text) for text in train_df['text']]\n",
        "\n",
        "embedding_size = 300\n",
        "fastText_model = FastText(sentences=tokenized, vector_size=embedding_size, window=3, min_count=2, sg=1)\n",
        "\n",
        "fastText_model.save('fastText_model.model')\n",
        "\"\"\"else:\n",
        "    fastText_model = FastText.load('fastText_model.model')\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2700ab33",
      "metadata": {
        "id": "2700ab33",
        "outputId": "93431420-804f-4b39-8a06-1ef88a053116",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2116 300\n"
          ]
        }
      ],
      "source": [
        "print(len(fastText_model.wv), len(fastText_model.wv[0]))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_sequence(text, fastText_model, max_seq_len=40, embedding_size=300):\n",
        "    tokens = simple_preprocess(text)\n",
        "    vectors = []\n",
        "    for token in text:\n",
        "        if token in fastText_model:\n",
        "            vectors.append(fastText_model[token])\n",
        "        else:\n",
        "            vectors.append(np.zeros(embedding_size))\n",
        "\n",
        "    if len(vectors) < max_seq_len:\n",
        "        padding = [np.zeros(embedding_size)] * (max_seq_len - len(vectors))\n",
        "        vectors += padding\n",
        "    else:\n",
        "        vectors = vectors[:max_seq_len]\n",
        "\n",
        "    emb = np.array(vectors)\n",
        "    seq=  [\n",
        "        word2idxLower.get(word, word2idxLower.get('<unk>'))\n",
        "        for word in text\n",
        "    ]\n",
        "    if len(seq) < max_seq_len:\n",
        "        padding = [0] * (max_seq_len - len(seq))\n",
        "        seq += padding\n",
        "    else:\n",
        "        seq = seq[:max_seq_len]\n",
        "    return emb, seq\n",
        "\n",
        "def generate_subword_embeddings(df, fastText_model, max_seq_len=40):\n",
        "    embeddings = []\n",
        "    embeddings_bng = []\n",
        "    for text in df['text']:\n",
        "        emb, emb_bng = text_to_sequence(text, fastText_model, max_seq_len)\n",
        "        embeddings.append(emb)\n",
        "        embeddings_bng.append(emb_bng)\n",
        "\n",
        "\n",
        "    return np.array(embeddings),np.array(embeddings_bng), np.array(df.drop(['text', 'category'],axis=1))"
      ],
      "metadata": {
        "id": "WV-Jhll7g9Hc"
      },
      "id": "WV-Jhll7g9Hc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51a2b479",
      "metadata": {
        "id": "51a2b479"
      },
      "outputs": [],
      "source": [
        "def text_to_sequence(text, fastText_model, max_seq_len=105, embedding_size=300):\n",
        "    tokens = simple_preprocess(text)\n",
        "    vectors = []\n",
        "    for token in tokens:\n",
        "        if token in fastText_model.wv:\n",
        "            vectors.append(fastText_model.wv[token])\n",
        "        else:\n",
        "            vectors.append(np.zeros(embedding_size))\n",
        "\n",
        "    if len(vectors) < max_seq_len:\n",
        "        padding = [np.zeros(embedding_size)] * (max_seq_len - len(vectors))\n",
        "        vectors += padding\n",
        "    else:\n",
        "        vectors = vectors[:max_seq_len]\n",
        "\n",
        "    emb = np.array(vectors)\n",
        "    seq=  [\n",
        "        word2idxLower.get(word, word2idxLower.get('<unk>'))\n",
        "        for word in tokens\n",
        "    ]\n",
        "    if len(seq) < max_seq_len:\n",
        "        padding = [0] * (max_seq_len - len(seq))\n",
        "        seq += padding\n",
        "    else:\n",
        "        seq = seq[:max_seq_len]\n",
        "    return emb,seq\n",
        "\n",
        "def generate_subword_embeddings(df, fastText_model, max_seq_len=105):\n",
        "    embeddings = []\n",
        "    embeddings_bng = []\n",
        "    for text in df['text']:\n",
        "        emb,emb_bng = text_to_sequence(text, fastText_model, max_seq_len)\n",
        "        embeddings.append(emb)\n",
        "        embeddings_bng.append(emb_bng)\n",
        "\n",
        "\n",
        "    return np.array(embeddings),np.array(embeddings_bng), np.array(df.drop(['text', 'category'],axis=1))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word2idxLower = {\n",
        "    word: index\n",
        "    for index, word in enumerate(vocab_npa)\n",
        "}\n",
        "\n",
        "# train_df = (\n",
        "#     train_df\n",
        "#     .map(lambda x: {\n",
        "#             'bang_emb': [\n",
        "#                 word2idxLower.get(simple_preprocess(text), word2idxLower['[UNK]'])\n",
        "#                 for text in x['text']\n",
        "#             ]\n",
        "#         }\n",
        "#     )\n",
        "# )\n",
        "# def pad_sequence(sequence, max_length, padding_value=0):\n",
        "#     return sequence + [padding_value] * (max_length - len(sequence))\n",
        "\n",
        "# def map_text_to_embeddings(text):\n",
        "#     tokens = simple_preprocess(text)\n",
        "#     seq=  [\n",
        "#         word2idxLower.get(word, word2idxLower.get('[UNK]'))\n",
        "#         for word in tokens\n",
        "#     ]\n",
        "#     return pad_sequence(seq,105)\n",
        "\n",
        "# def text_to_sequence(text):\n",
        "#     tokens = simple_preprocess(text)\n",
        "#     max_seq_len=105\n",
        "#     vectors = []\n",
        "#     for token in tokens:\n",
        "#         if token in fastText_model.wv:\n",
        "#             vectors.append(fastText_model.wv[token])\n",
        "#         else:\n",
        "#             vectors.append(np.zeros(embedding_size))\n",
        "\n",
        "#     if len(vectors) < max_seq_len:\n",
        "#         padding = [np.zeros(embedding_size)] * (max_seq_len - len(vectors))\n",
        "#         vectors += padding\n",
        "#     else:\n",
        "#         vectors = vectors[:max_seq_len]\n",
        "\n",
        "#     return np.array(vectors)\n",
        "\n",
        "# Apply the function to each row in the DataFrame\n",
        "# train_df['bng_emb'] = train_df['text'].apply(map_text_to_embeddings)\n",
        "# val_df['bng_emb'] = val_df['text'].apply(map_text_to_embeddings)\n",
        "# test_df['bng_emb'] = test_df['text'].apply(map_text_to_embeddings)\n",
        "\n",
        "# max_seq_length = train_df['bng_emb'].apply(len).max()\n",
        "\n",
        "# train_df['emb'] = train_df['text'].apply(text_to_sequence)\n",
        "# val_df['emb'] = val_df['text'].apply(text_to_sequence)\n",
        "# test_df['emb'] = test_df['text'].apply(text_to_sequence)"
      ],
      "metadata": {
        "id": "54dQ-TU8kuxf"
      },
      "id": "54dQ-TU8kuxf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3cafe491",
      "metadata": {
        "id": "3cafe491"
      },
      "outputs": [],
      "source": [
        "train_embeddings,train_embeddings_bng, train_fn = generate_subword_embeddings(train_df, fastText_model)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_embeddings.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4oxAe4schyjV",
        "outputId": "9efda804-2061-4421-b551-29a5cb77f0a2"
      },
      "id": "4oxAe4schyjV",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2792, 40, 300)"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_embeddings_bng"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8WpQ4PNlrXz",
        "outputId": "b0c63f9d-5025-4245-a62e-0af8bfc8f816"
      },
      "id": "P8WpQ4PNlrXz",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[103,  40, 496, ...,   8,  17, 116],\n",
              "       [ 17,  66,  17, ...,   1, 171,  17],\n",
              "       [363,  17,  89, ...,  32,  27,  11],\n",
              "       ...,\n",
              "       [ 43,  11,   4, ...,   0,   0,   0],\n",
              "       [  4,  17, 103, ..., 171,   1, 176],\n",
              "       [  4,  17, 103, ..., 103, 218,   1]])"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4dac999",
      "metadata": {
        "id": "c4dac999"
      },
      "outputs": [],
      "source": [
        "val_embeddings,val_embeddings_bng, val_fn = generate_subword_embeddings(val_df, fastText_model)\n",
        "test_embeddings ,test_embeddings_bng, test_fn = generate_subword_embeddings(test_df, fastText_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bf42a3f",
      "metadata": {
        "id": "6bf42a3f"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "train_embeddings = torch.Tensor(train_embeddings)\n",
        "train_embeddings_bng = torch.Tensor(train_embeddings_bng)\n",
        "train_fn = torch.Tensor(train_fn)\n",
        "train_labels = torch.Tensor(train_df['category'].values)\n",
        "\n",
        "val_embeddings = torch.Tensor(val_embeddings)\n",
        "val_embeddings_bng = torch.Tensor(val_embeddings_bng)\n",
        "val_fn = torch.Tensor(val_fn)\n",
        "val_labels = torch.Tensor(val_df['category'].values)\n",
        "\n",
        "test_embeddings = torch.Tensor(test_embeddings)\n",
        "test_embeddings_bng = torch.Tensor(test_embeddings_bng)\n",
        "test_fn = torch.Tensor(test_fn)\n",
        "test_labels = torch.Tensor(test_df['category'].values)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import TensorDataset, DataLoader"
      ],
      "metadata": {
        "id": "94PzXpuarZMm"
      },
      "id": "94PzXpuarZMm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d342c4b",
      "metadata": {
        "id": "0d342c4b"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "train_data = TensorDataset(train_embeddings, train_embeddings_bng , train_fn, train_labels)\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "val_data = TensorDataset(val_embeddings, val_embeddings_bng , val_fn, val_labels)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "test_data = TensorDataset(test_embeddings, test_embeddings_bng , test_fn, test_labels)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3041c771",
      "metadata": {
        "id": "3041c771"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "import copy\n",
        "\n",
        "def train_with_early_stopping(model, train_loader=train_loader, val_loader=val_loader, num_epochs=50, patience=10):\n",
        "    best_val_loss = float('inf')\n",
        "    best_val_f1 = 0\n",
        "    current_patience = 0\n",
        "    best_model = None\n",
        "\n",
        "    for epoch in tqdm(range(num_epochs)):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for inputs, inputs_bng, inputs_fn, labels in train_loader:\n",
        "            inputs,inputs_bng, inputs_fn, labels = inputs.to(device),inputs_bng.to(device), inputs_fn.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            out = model(inputs,inputs_bng, inputs_fn)\n",
        "            loss = criterion(out, labels.long())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        mean_loss = total_loss / len(train_loader)\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        val_predictions = []\n",
        "        val_targets = []\n",
        "        correct, total = 0, 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs,inputs_bng, inputs_fn, labels in val_loader:\n",
        "                inputs,inputs_bng, inputs_fn, labels = inputs.to(device),inputs_bng.to(device), inputs_fn.to(device), labels.to(device)\n",
        "                out = model(inputs,inputs_bng, inputs_fn)\n",
        "\n",
        "                loss = criterion(out, labels.long())\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                _, predicted = torch.max(out, 1)\n",
        "\n",
        "                val_predictions.extend(predicted.cpu().long().numpy())\n",
        "                val_targets.extend(labels.cpu().numpy())\n",
        "\n",
        "                # print(len(val_targets), val_targets[-10:])\n",
        "                # print(len(val_predictions), val_predictions[-10:])\n",
        "\n",
        "        # val_accuracy = accuracy_score(val_targets, val_predictions)\n",
        "        val_loss /= len(val_loader)\n",
        "        val_w_f1 = f1_score(val_targets, val_predictions, average='weighted')\n",
        "\n",
        "        print(f'Epoch [{epoch + 1}/{num_epochs}], Train Loss: {mean_loss}, Validation Loss: {val_loss}, Validation F1: {val_w_f1}')\n",
        "\n",
        "        if val_w_f1 > best_val_f1:\n",
        "            best_val_f1 = val_w_f1\n",
        "            current_patience = 0\n",
        "            best_model = copy.deepcopy(model)\n",
        "        else:\n",
        "            current_patience += 1\n",
        "\n",
        "        if current_patience >= patience:\n",
        "            print(f'Stopping after {epoch+1} epochs')\n",
        "            break\n",
        "\n",
        "    return best_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28101eae",
      "metadata": {
        "id": "28101eae"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score, recall_score, precision_score, classification_report\n",
        "\n",
        "def test_metrics(model, test_loader):\n",
        "    model.eval()\n",
        "    test_predictions = []\n",
        "    test_targets = []\n",
        "    with torch.no_grad():\n",
        "        for inputs,inputs_bng, inputs_fn, labels in test_loader:\n",
        "            inputs,inputs_bng, inputs_fn, labels = inputs.to(device),inputs_bng.to(device), inputs_fn.to(device), labels.to(device)\n",
        "            out = model(inputs,inputs_bng, inputs_fn)\n",
        "\n",
        "            _, predicted = torch.max(out, 1)\n",
        "            test_predictions.extend(predicted.cpu().long().numpy())\n",
        "            test_targets.extend(labels.cpu().numpy())\n",
        "\n",
        "    print(len(test_targets), len(test_predictions))\n",
        "\n",
        "    test_acc = accuracy_score(test_targets, test_predictions)\n",
        "    test_w_f1 = f1_score(test_targets, test_predictions, average='weighted')\n",
        "    test_macro_f1 = f1_score(test_targets, test_predictions, average='macro')\n",
        "    class_report = classification_report(test_targets, test_predictions)\n",
        "\n",
        "    print('Accuracy:', test_acc)\n",
        "    print('F1-Weighted:', test_w_f1)\n",
        "    print('F1-Macro:', test_macro_f1)\n",
        "    print('Classification Report:', class_report, sep='\\n')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "import copy\n",
        "\n",
        "def maori_train_with_early_stopping(model, train_loader=train_loader, val_loader=val_loader, num_epochs=50, patience=10):\n",
        "    best_val_loss = float('inf')\n",
        "    best_val_f1 = 0\n",
        "    current_patience = 0\n",
        "    best_model = None\n",
        "\n",
        "    for epoch in tqdm(range(num_epochs)):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for inputs,inputs_bng, inputs_fn, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            out = model(inputs)\n",
        "            loss = criterion(out, labels.long())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        mean_loss = total_loss / len(train_loader)\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        val_predictions = []\n",
        "        val_targets = []\n",
        "        correct, total = 0, 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs,inputs_bng, inputs_fn, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                out = model(inputs)\n",
        "\n",
        "                loss = criterion(out, labels.long())\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                _, predicted = torch.max(out, 1)\n",
        "\n",
        "                val_predictions.extend(predicted.cpu().long().numpy())\n",
        "                val_targets.extend(labels.cpu().numpy())\n",
        "\n",
        "                # print(len(val_targets), val_targets[-10:])\n",
        "                # print(len(val_predictions), val_predictions[-10:])\n",
        "\n",
        "        # val_accuracy = accuracy_score(val_targets, val_predictions)\n",
        "        val_loss /= len(val_loader)\n",
        "        val_w_f1 = f1_score(val_targets, val_predictions, average='weighted')\n",
        "\n",
        "        print(f'Epoch [{epoch + 1}/{num_epochs}], Train Loss: {mean_loss}, Validation Loss: {val_loss}, Validation F1: {val_w_f1}')\n",
        "\n",
        "        if val_w_f1 > best_val_f1:\n",
        "            best_val_f1 = val_w_f1\n",
        "            current_patience = 0\n",
        "            best_model = copy.deepcopy(model)\n",
        "        else:\n",
        "            current_patience += 1\n",
        "\n",
        "        if current_patience >= patience:\n",
        "            print(f'Stopping after {epoch+1} epochs')\n",
        "            break\n",
        "\n",
        "    return best_model"
      ],
      "metadata": {
        "id": "SO7rmLjMKZw9"
      },
      "id": "SO7rmLjMKZw9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, recall_score, precision_score, classification_report\n",
        "\n",
        "def maori_test_metrics(model, test_loader):\n",
        "    model.eval()\n",
        "    test_predictions = []\n",
        "    test_targets = []\n",
        "    with torch.no_grad():\n",
        "        for inputs,inputs_bng, inputs_fn, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            out = model(inputs)\n",
        "\n",
        "            _, predicted = torch.max(out, 1)\n",
        "            test_predictions.extend(predicted.cpu().long().numpy())\n",
        "            test_targets.extend(labels.cpu().numpy())\n",
        "\n",
        "    print(len(test_targets), len(test_predictions))\n",
        "\n",
        "    test_acc = accuracy_score(test_targets, test_predictions)\n",
        "    test_w_f1 = f1_score(test_targets, test_predictions, average='weighted')\n",
        "    test_macro_f1 = f1_score(test_targets, test_predictions, average='macro')\n",
        "    class_report = classification_report(test_targets, test_predictions)\n",
        "\n",
        "    print('Accuracy:', test_acc)\n",
        "    print('F1-Weighted:', test_w_f1)\n",
        "    print('F1-Macro:', test_macro_f1)\n",
        "    print('Classification Report:', class_report, sep='\\n')"
      ],
      "metadata": {
        "id": "ElAJTEfnLfMq"
      },
      "id": "ElAJTEfnLfMq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gmf6ntRyLLYK",
        "outputId": "2bd3b245-6c98-4b68-b5b7-aed4b827bdeb"
      },
      "id": "Gmf6ntRyLLYK",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maori(\n",
            "  (embedding): Embedding(7002, 128)\n",
            "  (lstm): LSTM(300, 12, batch_first=True, bidirectional=True)\n",
            "  (drop): Dropout(p=0.0, inplace=False)\n",
            "  (linear): Linear(in_features=24, out_features=3, bias=True)\n",
            "  (softmax): Softmax(dim=0)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "maori_train_with_early_stopping(model, num_epochs=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hKib6m0hLDmt",
        "outputId": "30f0ef81-885d-40b6-f50e-0a2abbd5bbad"
      },
      "id": "hKib6m0hLDmt",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|▏         | 1/50 [00:00<00:24,  2.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Train Loss: 1.065773897821253, Validation Loss: 1.060996515410287, Validation F1: 0.35065572620800894\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  4%|▍         | 2/50 [00:00<00:23,  2.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/50], Train Loss: 1.0659996953877535, Validation Loss: 1.060996515410287, Validation F1: 0.35065572620800894\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  6%|▌         | 3/50 [00:01<00:22,  2.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/50], Train Loss: 1.0659996910528704, Validation Loss: 1.060996515410287, Validation F1: 0.35065572620800894\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  8%|▊         | 4/50 [00:01<00:21,  2.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/50], Train Loss: 1.0661010438745673, Validation Loss: 1.060996515410287, Validation F1: 0.35065572620800894\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|█         | 5/50 [00:02<00:21,  2.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/50], Train Loss: 1.0661010427908464, Validation Loss: 1.060996515410287, Validation F1: 0.35065572620800894\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 12%|█▏        | 6/50 [00:02<00:20,  2.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/50], Train Loss: 1.0659996997226369, Validation Loss: 1.060996515410287, Validation F1: 0.35065572620800894\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 14%|█▍        | 7/50 [00:03<00:19,  2.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7/50], Train Loss: 1.065999687801708, Validation Loss: 1.060996515410287, Validation F1: 0.35065572620800894\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 16%|█▌        | 8/50 [00:03<00:19,  2.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [8/50], Train Loss: 1.0661010482094504, Validation Loss: 1.060996515410287, Validation F1: 0.35065572620800894\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 18%|█▊        | 9/50 [00:04<00:18,  2.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [9/50], Train Loss: 1.0659996943040329, Validation Loss: 1.060996515410287, Validation F1: 0.35065572620800894\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 10/50 [00:04<00:18,  2.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/50], Train Loss: 1.0654467636888678, Validation Loss: 1.060996515410287, Validation F1: 0.35065572620800894\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 10/50 [00:05<00:20,  1.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [11/50], Train Loss: 1.0657739086584612, Validation Loss: 1.060996515410287, Validation F1: 0.35065572620800894\n",
            "Stopping after 11 epochs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Maori(\n",
              "  (embedding): Embedding(7002, 128)\n",
              "  (lstm): LSTM(300, 12, batch_first=True, bidirectional=True)\n",
              "  (drop): Dropout(p=0.0, inplace=False)\n",
              "  (linear): Linear(in_features=24, out_features=3, bias=True)\n",
              "  (softmax): Softmax(dim=0)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 185
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "maori_test_metrics(model, val_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3zalqCHLu8n",
        "outputId": "ce43af3c-ae15-4938-d3d6-9c5a5dd946bc"
      },
      "id": "t3zalqCHLu8n",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "194 194\n",
            "Accuracy: 0.5154639175257731\n",
            "F1-Weighted: 0.35065572620800894\n",
            "F1-Macro: 0.22675736961451246\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.52      1.00      0.68       100\n",
            "         1.0       0.00      0.00      0.00        51\n",
            "         2.0       0.00      0.00      0.00        43\n",
            "\n",
            "    accuracy                           0.52       194\n",
            "   macro avg       0.17      0.33      0.23       194\n",
            "weighted avg       0.27      0.52      0.35       194\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "maori_test_metrics(model, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9GPBtm93Lz9b",
        "outputId": "a10ae2a4-ad1b-416d-93a7-196e73536def"
      },
      "id": "9GPBtm93Lz9b",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "194 194\n",
            "Accuracy: 0.5567010309278351\n",
            "F1-Weighted: 0.39817027377619985\n",
            "F1-Macro: 0.23841059602649006\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.56      1.00      0.72       108\n",
            "         1.0       0.00      0.00      0.00        47\n",
            "         2.0       0.00      0.00      0.00        39\n",
            "\n",
            "    accuracy                           0.56       194\n",
            "   macro avg       0.19      0.33      0.24       194\n",
            "weighted avg       0.31      0.56      0.40       194\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ebe1d5b",
      "metadata": {
        "id": "4ebe1d5b",
        "outputId": "d98a79c4-d4d3-4c9c-bb43-c5be772287dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maori(\n",
            "  (embedding): Embedding(7002, 128)\n",
            "  (lstm): LSTM(300, 12, batch_first=True, bidirectional=True)\n",
            "  (drop): Dropout(p=0.0, inplace=False)\n",
            "  (linear): Linear(in_features=24, out_features=3, bias=True)\n",
            "  (softmax): Softmax(dim=0)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "class Maori(nn.Module): #Ignore\n",
        "    def __init__(self, input_size=300, hidden_size=128, out_size=3, dropout_prob=0.5):\n",
        "        super(Maori, self).__init__()\n",
        "        self.embedding = nn.Embedding.from_pretrained(torch.from_numpy(embs_npa).float())\n",
        "        self.embedding = nn.Embedding.from_pretrained(torch.from_numpy(embs_npa).float(), freeze=True)\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True, bidirectional=True)\n",
        "        self.drop = nn.Dropout(dropout_prob)\n",
        "        self.linear = nn.Linear(2*hidden_size, out_size)\n",
        "        self.softmax = nn.Softmax(dim=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.drop(x)\n",
        "        out, _ = self.lstm(x)\n",
        "        # out = self.drop(out)\n",
        "\n",
        "        out = self.linear(out[:, -1, :])\n",
        "\n",
        "        return out\n",
        "\n",
        "model = Maori(out_size=len(idx2label), hidden_size=12, dropout_prob=0.)\n",
        "model = model.to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model(batch[0].to(device))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGnum5mrKOL_",
        "outputId": "39d11133-7b82-40e9-9339-a668270cea12"
      },
      "id": "YGnum5mrKOL_",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 179
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.query = nn.Linear(input_dim, input_dim)\n",
        "        self.key = nn.Linear(input_dim, input_dim)\n",
        "        self.value = nn.Linear(input_dim, input_dim)\n",
        "        self.softmax = nn.Softmax(dim=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        queries = self.query(x)\n",
        "        keys = self.key(x)\n",
        "        values = self.value(x)\n",
        "        scores = torch.bmm(queries, keys.transpose(1, 2)) / (self.input_dim ** 0.5)\n",
        "        attention = self.softmax(scores)\n",
        "        weighted = torch.bmm(attention, values)\n",
        "        return weighted\n"
      ],
      "metadata": {
        "id": "h-13KjLe7ybz"
      },
      "id": "h-13KjLe7ybz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MukherjeeV1(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(MukherjeeV1, self).__init__()\n",
        "\n",
        "        # Word Embeddings\n",
        "        self.lstm_word = nn.LSTM(config['word_input_size'], config['lstm_word_hidden'], num_layers=2, batch_first=True, bidirectional=True)\n",
        "        # self.dropout_word = nn.Dropout(config[\"drop_word\"])\n",
        "        self.attention_word = SelfAttention(config['lstm_word_hidden']*2)\n",
        "        # self.multihead_attn_word = nn.MultiheadAttention(config['lstm_word_hidden']*2, config['num_heads'])\n",
        "\n",
        "        # Char Embeddings\n",
        "        # self.lstm_char = nn.LSTM(config['input_size'], config['lstm_char_hidden'], batch_first=True, bidirectional=True)\n",
        "        # self.dropout_char = nn.Dropout(config[\"drop_char\"])\n",
        "\n",
        "        #Bangla Embeddings\n",
        "        self.embedding = nn.Embedding.from_pretrained(torch.from_numpy(embs_npa).float(), freeze=True)\n",
        "        self.lstm = nn.LSTM(input_size = config['subword_input_size'], hidden_size = config['lstm_subword_hidden'], num_layers=4, bidirectional=True, batch_first=True)\n",
        "        # self.linear = nn.Linear(config['hidden_dim'] * 2,  config['num_heads'])\n",
        "        # self.dropout = nn.Dropout(config['drop_char'])\n",
        "        self.attention_subword = SelfAttention(config['lstm_subword_hidden']*2)\n",
        "        # self.multihead_attn_subword = nn.MultiheadAttention(config['hidden_dim']*2, config['num_heads'])\n",
        "\n",
        "        # Dense Layers\n",
        "        self.attention_both = SelfAttention(config['lstm_word_hidden']*2+config['lstm_subword_hidden']*2)\n",
        "        # self.multihead_attn_both = nn.MultiheadAttention(config['lstm_word_hidden']*2+config['hidden_dim']*2, config['num_heads'])\n",
        "\n",
        "        # self.dropout_fc1 = nn.Dropout(config[\"drop_fc\"])\n",
        "        # self.dropout_fc2 = nn.Dropout(config[\"drop_fc\"])\n",
        "        self.fc1 = nn.Linear(config['fc1_dim'], config['fc2_dim'])\n",
        "        self.fc2 = nn.Linear(config['fc2_dim'], config['fc3_dim'])\n",
        "        self.fc3 = nn.Linear(config['fc3_dim'], config['n_classes'])\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x, x_bng, fn):\n",
        "        # Word Embeddings\n",
        "        out, (hidden,_) = self.lstm_word(x)\n",
        "        # out = self.dropout_word(out)\n",
        "        out_word = self.attention_word(out)\n",
        "        # out_word, _ = self.multihead_attn_word(out, out, out)\n",
        "\n",
        "        # Char Embeddings\n",
        "        # out, (hidden,_) = self.lstm_char(x)\n",
        "        # out_char = self.dropout_char(out)\n",
        "        out_subword = self.embedding(x_bng.long())\n",
        "        # print(x[0])\n",
        "        # print(embedded[0])\n",
        "        # print(embedded.shape)\n",
        "        out_subword, (hidden, _) = self.lstm(out_subword)\n",
        "        # embedded = self.dropout(embedded)\n",
        "        out_subword = self.attention_subword(out_subword)\n",
        "        # out, _ = self.multihead_attn_subword(embedded, embedded, embedded)\n",
        "        # outputs = self.linear(outputs)\n",
        "        # embedded =  outputs.transpose(1, 2)\n",
        "\n",
        "        # print(x[0])\n",
        "        # print(embedded[0])\n",
        "\n",
        "\n",
        "\n",
        "        # Concatenate Word and Char Embeddings\n",
        "        # print(out_word.shape)\n",
        "        # print(embedded.shape)\n",
        "        out = torch.cat((out_word, out_subword), dim=2)\n",
        "        # print(out.shape)\n",
        "        # print(out[:, -1, :].shape)\n",
        "        # expected_input_size = config['lstm_word_hidden']*2 + config['hidden_dim']*2\n",
        "        # print(f\"Expected input size of fully connected layer: {expected_input_size}\")\n",
        "        # out = out_word\n",
        "\n",
        "        # Attention\n",
        "        out = self.attention_both(out)\n",
        "        # out = self.multihead_attn_both(out,out,out)\n",
        "        # print(out.shape)\n",
        "        # print(out[:, -1, :].shape)\n",
        "        # print(fn.shape)\n",
        "        out = torch.cat((out[:, -1, :], fn), dim=1)\n",
        "        # print(out.shape)\n",
        "        # x = torch.cat((x, any_caps.unsqueeze(2)), dim=2)\n",
        "        # Dense Layers\n",
        "        out = self.relu(self.fc1(out))\n",
        "        # out = self.dropout_fc1(out)\n",
        "        out = self.relu(self.fc2(out))\n",
        "        # out = self.dropout_fc2(out)\n",
        "        out = self.relu(self.fc3(out))\n",
        "        return out"
      ],
      "metadata": {
        "id": "Uid3HLQFl6cD"
      },
      "id": "Uid3HLQFl6cD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config={'word_input_size':300,\n",
        "        'subword_input_size':128,\n",
        "        'lstm_word_hidden':300,\n",
        "        'lstm_subword_hidden':128,\n",
        "        # 'lstm_char_hidden':128,\n",
        "        'fc1_dim':300*2+128*2+train_fn.shape[1],\n",
        "        'fc2_dim':512,\n",
        "        'fc3_dim':128,\n",
        "        'n_classes':len(idx2label),\n",
        "        'drop_subword':0.2,\n",
        "        'drop_word':0.4,\n",
        "        'drop_fc':0.4,\n",
        "        # 'num_heads':8,\n",
        "        # 'embedding_dim' :128,\n",
        "        # 'hidden_dim' : 256,\n",
        "        }\n",
        "\n",
        "model = MukherjeeV1(config)\n",
        "model = model.to(device)\n",
        "print(model)"
      ],
      "metadata": {
        "id": "073Z343hAgIj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa0ee94b-7a16-4888-ab9c-37b7eacf3d53"
      },
      "id": "073Z343hAgIj",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MukherjeeV1(\n",
            "  (lstm_word): LSTM(300, 300, num_layers=2, batch_first=True, bidirectional=True)\n",
            "  (attention_word): SelfAttention(\n",
            "    (query): Linear(in_features=600, out_features=600, bias=True)\n",
            "    (key): Linear(in_features=600, out_features=600, bias=True)\n",
            "    (value): Linear(in_features=600, out_features=600, bias=True)\n",
            "    (softmax): Softmax(dim=2)\n",
            "  )\n",
            "  (embedding): Embedding(7001, 128)\n",
            "  (lstm): LSTM(128, 128, num_layers=4, batch_first=True, bidirectional=True)\n",
            "  (attention_subword): SelfAttention(\n",
            "    (query): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (key): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (value): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (softmax): Softmax(dim=2)\n",
            "  )\n",
            "  (attention_both): SelfAttention(\n",
            "    (query): Linear(in_features=856, out_features=856, bias=True)\n",
            "    (key): Linear(in_features=856, out_features=856, bias=True)\n",
            "    (value): Linear(in_features=856, out_features=856, bias=True)\n",
            "    (softmax): Softmax(dim=2)\n",
            "  )\n",
            "  (fc1): Linear(in_features=862, out_features=512, bias=True)\n",
            "  (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
            "  (fc3): Linear(in_features=128, out_features=3, bias=True)\n",
            "  (relu): ReLU()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77a654fd",
      "metadata": {
        "id": "77a654fd"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# class_weights = [1.0,5.0,5.0]\n",
        "# criterion = nn.CrossEntropyLoss(weight=torch.tensor(class_weights).to(device))\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adamax(model.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f36920d",
      "metadata": {
        "id": "3f36920d",
        "outputId": "eb87952b-72ed-4a83-fe1b-d1af0b387287",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|▏         | 1/50 [00:02<02:16,  2.78s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Train Loss: 1.0409886864098636, Validation Loss: 0.9885019779205322, Validation F1: 0.3459122939043579\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  4%|▍         | 2/50 [00:05<02:13,  2.78s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/50], Train Loss: 0.9620972722768784, Validation Loss: 0.949260675907135, Validation F1: 0.3459122939043579\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  6%|▌         | 3/50 [00:08<02:10,  2.78s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/50], Train Loss: 0.924645181406628, Validation Loss: 0.9481417775154114, Validation F1: 0.3459122939043579\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  8%|▊         | 4/50 [00:11<02:07,  2.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/50], Train Loss: 0.8915377272800966, Validation Loss: 0.9743558287620544, Validation F1: 0.3459122939043579\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|█         | 5/50 [00:13<02:05,  2.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/50], Train Loss: 0.8652920953252099, Validation Loss: 0.9198076248168945, Validation F1: 0.5503222868498677\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 12%|█▏        | 6/50 [00:16<02:03,  2.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/50], Train Loss: 0.7987961836836555, Validation Loss: 0.9201187372207642, Validation F1: 0.56455475088653\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 14%|█▍        | 7/50 [00:19<02:01,  2.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7/50], Train Loss: 0.7619060386310924, Validation Loss: 0.9878057658672332, Validation F1: 0.5566524013939022\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 16%|█▌        | 8/50 [00:22<01:58,  2.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [8/50], Train Loss: 0.7164708281105215, Validation Loss: 0.9031160414218903, Validation F1: 0.5760344063642018\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 18%|█▊        | 9/50 [00:25<01:55,  2.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [9/50], Train Loss: 0.6658594350923192, Validation Loss: 1.04158273935318, Validation F1: 0.5456627291525774\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 10/50 [00:28<01:52,  2.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/50], Train Loss: 0.6420487084171989, Validation Loss: 0.9452392935752869, Validation F1: 0.5279031517181086\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 22%|██▏       | 11/50 [00:30<01:49,  2.80s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [11/50], Train Loss: 0.6020404574545947, Validation Loss: 1.0265158712863922, Validation F1: 0.5243269977560268\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 24%|██▍       | 12/50 [00:33<01:46,  2.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [12/50], Train Loss: 0.5476078682325103, Validation Loss: 1.201272976398468, Validation F1: 0.5520326136885623\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 26%|██▌       | 13/50 [00:36<01:43,  2.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [13/50], Train Loss: 0.5403898947618224, Validation Loss: 1.2153971374034882, Validation F1: 0.5899248185797413\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 28%|██▊       | 14/50 [00:39<01:40,  2.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [14/50], Train Loss: 0.4694282893430103, Validation Loss: 1.1040640115737914, Validation F1: 0.6005811854119022\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|███       | 15/50 [00:41<01:37,  2.78s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [15/50], Train Loss: 0.4214337776330384, Validation Loss: 1.2181549072265625, Validation F1: 0.586568744338237\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 32%|███▏      | 16/50 [00:44<01:34,  2.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [16/50], Train Loss: 0.37404588068073447, Validation Loss: 1.738259506225586, Validation F1: 0.582070739974335\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 34%|███▍      | 17/50 [00:47<01:31,  2.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [17/50], Train Loss: 0.3428217463872649, Validation Loss: 1.343940508365631, Validation F1: 0.5953760062885959\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 36%|███▌      | 18/50 [00:50<01:28,  2.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [18/50], Train Loss: 0.31241255693814973, Validation Loss: 1.5404184699058532, Validation F1: 0.6353633479248303\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 38%|███▊      | 19/50 [00:52<01:25,  2.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [19/50], Train Loss: 0.2538599561561238, Validation Loss: 1.6463610351085662, Validation F1: 0.623336378694711\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 20/50 [00:55<01:22,  2.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [20/50], Train Loss: 0.2507292553782463, Validation Loss: 2.037303149700165, Validation F1: 0.5753555908754631\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 42%|████▏     | 21/50 [00:58<01:19,  2.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [21/50], Train Loss: 0.23604725335131993, Validation Loss: 1.6930459976196288, Validation F1: 0.5870454287048815\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 44%|████▍     | 22/50 [01:01<01:17,  2.75s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [22/50], Train Loss: 0.19409146667881447, Validation Loss: 1.7017766892910005, Validation F1: 0.5955664110808804\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 46%|████▌     | 23/50 [01:03<01:14,  2.75s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [23/50], Train Loss: 0.16308119867674328, Validation Loss: 2.271748161315918, Validation F1: 0.5972193424730821\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 48%|████▊     | 24/50 [01:06<01:11,  2.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [24/50], Train Loss: 0.11171280673112381, Validation Loss: 2.83335394859314, Validation F1: 0.6035173552574014\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 25/50 [01:09<01:09,  2.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [25/50], Train Loss: 0.13244913450696252, Validation Loss: 2.5762812376022337, Validation F1: 0.6004081119733229\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 52%|█████▏    | 26/50 [01:12<01:06,  2.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [26/50], Train Loss: 0.09426060731692071, Validation Loss: 2.5214628100395204, Validation F1: 0.5909414071549663\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 54%|█████▍    | 27/50 [01:15<01:03,  2.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [27/50], Train Loss: 0.1107005953619426, Validation Loss: 2.2050605177879334, Validation F1: 0.6056771604646883\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 54%|█████▍    | 27/50 [01:17<01:06,  2.88s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [28/50], Train Loss: 0.09504386971027336, Validation Loss: 2.44401193857193, Validation F1: 0.6229067289157211\n",
            "Stopping after 28 epochs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "best_model = train_with_early_stopping(model, num_epochs=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68d857bc",
      "metadata": {
        "id": "68d857bc",
        "outputId": "7ccf9822-f1da-4aaa-df3b-4e6cb26ad3df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:879: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "776 776\n",
            "Accuracy: 0.8079896907216495\n",
            "F1-Weighted: 0.8081453284750719\n",
            "F1-Macro: 0.7788914508851073\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.85      0.86      0.85       395\n",
            "         1.0       0.80      0.78      0.79       265\n",
            "         2.0       0.68      0.71      0.69       116\n",
            "\n",
            "    accuracy                           0.81       776\n",
            "   macro avg       0.78      0.78      0.78       776\n",
            "weighted avg       0.81      0.81      0.81       776\n",
            "\n"
          ]
        }
      ],
      "source": [
        "test_metrics(best_model, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_metrics(best_model, val_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARsPGoxQGOmx",
        "outputId": "23d1d30c-a60f-49dc-a671-d67b3f5b1ce0"
      },
      "id": "ARsPGoxQGOmx",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "311 311\n",
            "Accuracy: 0.6366559485530546\n",
            "F1-Weighted: 0.6353633479248303\n",
            "F1-Macro: 0.5940871795485457\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.70      0.71      0.70       159\n",
            "         1.0       0.60      0.60      0.60       106\n",
            "         2.0       0.50      0.46      0.48        46\n",
            "\n",
            "    accuracy                           0.64       311\n",
            "   macro avg       0.60      0.59      0.59       311\n",
            "weighted avg       0.63      0.64      0.64       311\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:879: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_metrics(best_model, train_loader)"
      ],
      "metadata": {
        "id": "KXMJGUb39saR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc308b52-3183-4933-8248-a82e8769d0b7"
      },
      "id": "KXMJGUb39saR",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:879: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2792 2792\n",
            "Accuracy: 0.9151146131805158\n",
            "F1-Weighted: 0.9159956118997861\n",
            "F1-Macro: 0.888767564974542\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.96      0.94      0.95      1496\n",
            "         1.0       0.91      0.89      0.90       927\n",
            "         2.0       0.77      0.86      0.81       369\n",
            "\n",
            "    accuracy                           0.92      2792\n",
            "   macro avg       0.88      0.90      0.89      2792\n",
            "weighted avg       0.92      0.92      0.92      2792\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(best_model.state_dict(), '/content/drive/MyDrive/nlp_project/MukherjeeV2.pt')"
      ],
      "metadata": {
        "id": "FAmIfvdg9sXH"
      },
      "id": "FAmIfvdg9sXH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OGUWAEXw9sTy"
      },
      "id": "OGUWAEXw9sTy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trials"
      ],
      "metadata": {
        "id": "MmuNNK4a9rJ7"
      },
      "id": "MmuNNK4a9rJ7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "307114fe",
      "metadata": {
        "id": "307114fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21116d50-e9a6-4421-f009-b44073e77949"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:879: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3491 3491\n",
            "Accuracy: 0.5124606130048697\n",
            "F1-Weighted: 0.39482327239999104\n",
            "F1-Macro: 0.27662668511280347\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.52      0.95      0.67      1766\n",
            "         1.0       0.43      0.10      0.16      1218\n",
            "         2.0       0.00      0.00      0.00       507\n",
            "\n",
            "    accuracy                           0.51      3491\n",
            "   macro avg       0.32      0.35      0.28      3491\n",
            "weighted avg       0.41      0.51      0.39      3491\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "# best_model, test_loader\n",
        "test_predictions = []\n",
        "test_targets = []\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        out = best_model(inputs)\n",
        "\n",
        "        _, predicted = torch.max(out, 1)\n",
        "        test_predictions.extend(predicted.cpu().long().numpy())\n",
        "        test_targets.extend(labels.cpu().numpy())\n",
        "\n",
        "print(len(test_targets), len(test_predictions))\n",
        "\n",
        "test_acc = accuracy_score(test_targets, test_predictions)\n",
        "test_w_f1 = f1_score(test_targets, test_predictions, average='weighted')\n",
        "test_macro_f1 = f1_score(test_targets, test_predictions, average='macro')\n",
        "class_report = classification_report(test_targets, test_predictions)\n",
        "\n",
        "print('Accuracy:', test_acc)\n",
        "print('F1-Weighted:', test_w_f1)\n",
        "print('F1-Macro:', test_macro_f1)\n",
        "print('Classification Report:', class_report, sep='\\n')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "set(test_predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IuwrSptrjcnJ",
        "outputId": "cecb4de2-08b2-4802-b409-e84ca976ee35"
      },
      "id": "IuwrSptrjcnJ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0}"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch = next(iter(train_loader))"
      ],
      "metadata": {
        "id": "1npc2XjtjkuC"
      },
      "id": "1npc2XjtjkuC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch[-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hEUknLjbRg3q",
        "outputId": "f826df12-3004-4bf3-e66d-3715564ca4d1"
      },
      "id": "hEUknLjbRg3q",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 1., 2., 1., 0., 0., 0., 0., 0., 2., 0., 1., 0., 1., 1., 0., 1., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 2., 1., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 320
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wa3k2r3uj0Lx",
        "outputId": "4fa46b2a-98bf-44c5-e5fb-145aef330031"
      },
      "id": "Wa3k2r3uj0Lx",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 105, 300])"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model(batch[0].to(device), batch[1].to(device), batch[2].to(device))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y085RPXTj1Ni",
        "outputId": "57c5a57a-c141-4b93-870d-d515600780d6"
      },
      "id": "Y085RPXTj1Ni",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 40, 856])\n",
            "torch.Size([64, 856])\n",
            "torch.Size([64, 6])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0040, 0.0768, 0.0788],\n",
              "        [0.0000, 0.0321, 0.0752],\n",
              "        [0.0000, 0.0217, 0.0738],\n",
              "        [0.0000, 0.0259, 0.0745],\n",
              "        [0.0000, 0.0260, 0.0746],\n",
              "        [0.0000, 0.0255, 0.0755],\n",
              "        [0.0000, 0.0253, 0.0797],\n",
              "        [0.0016, 0.0674, 0.0769],\n",
              "        [0.0000, 0.0369, 0.0741],\n",
              "        [0.0000, 0.0258, 0.0761],\n",
              "        [0.0000, 0.0258, 0.0744],\n",
              "        [0.0000, 0.0346, 0.0757],\n",
              "        [0.0000, 0.0219, 0.0671],\n",
              "        [0.0000, 0.0388, 0.0693],\n",
              "        [0.0000, 0.0210, 0.0800],\n",
              "        [0.0000, 0.0257, 0.0744],\n",
              "        [0.0000, 0.0220, 0.0720],\n",
              "        [0.0000, 0.0323, 0.0751],\n",
              "        [0.0000, 0.0261, 0.0745],\n",
              "        [0.0000, 0.0321, 0.0751],\n",
              "        [0.0000, 0.0217, 0.0737],\n",
              "        [0.0000, 0.0216, 0.0737],\n",
              "        [0.0000, 0.0321, 0.0751],\n",
              "        [0.0000, 0.0462, 0.0763],\n",
              "        [0.0000, 0.0214, 0.0744],\n",
              "        [0.0000, 0.0321, 0.0703],\n",
              "        [0.0000, 0.0531, 0.0758],\n",
              "        [0.0000, 0.0216, 0.0738],\n",
              "        [0.0000, 0.0217, 0.0738],\n",
              "        [0.0000, 0.0261, 0.0745],\n",
              "        [0.0000, 0.0219, 0.0719],\n",
              "        [0.0000, 0.0295, 0.0727],\n",
              "        [0.0000, 0.0380, 0.0767],\n",
              "        [0.0000, 0.0210, 0.0750],\n",
              "        [0.0000, 0.0313, 0.0716],\n",
              "        [0.0000, 0.0390, 0.0782],\n",
              "        [0.0000, 0.0268, 0.0710],\n",
              "        [0.0000, 0.0272, 0.0739],\n",
              "        [0.0000, 0.0216, 0.0726],\n",
              "        [0.0000, 0.0347, 0.0757],\n",
              "        [0.0000, 0.0409, 0.0757],\n",
              "        [0.0000, 0.0259, 0.0744],\n",
              "        [0.0000, 0.0386, 0.0705],\n",
              "        [0.0000, 0.0394, 0.0759],\n",
              "        [0.0000, 0.0391, 0.0782],\n",
              "        [0.0000, 0.0293, 0.0727],\n",
              "        [0.0000, 0.0321, 0.0754],\n",
              "        [0.0000, 0.0312, 0.0700],\n",
              "        [0.0000, 0.0320, 0.0757],\n",
              "        [0.0000, 0.0252, 0.0795],\n",
              "        [0.0000, 0.0317, 0.0746],\n",
              "        [0.0000, 0.0605, 0.0761],\n",
              "        [0.0000, 0.0262, 0.0745],\n",
              "        [0.0000, 0.0237, 0.0779],\n",
              "        [0.0000, 0.0258, 0.0744],\n",
              "        [0.0000, 0.0320, 0.0750],\n",
              "        [0.0000, 0.0259, 0.0744],\n",
              "        [0.0000, 0.0217, 0.0737],\n",
              "        [0.0000, 0.0270, 0.0710],\n",
              "        [0.0000, 0.0434, 0.0775],\n",
              "        [0.0000, 0.0259, 0.0744],\n",
              "        [0.0000, 0.0291, 0.0690],\n",
              "        [0.0000, 0.0429, 0.0756],\n",
              "        [0.0000, 0.0431, 0.0753]], device='cuda:0', grad_fn=<ReluBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LvjWZcKA90Cr"
      },
      "id": "LvjWZcKA90Cr",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}